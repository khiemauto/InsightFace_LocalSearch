{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import json\n",
    "import threading\n",
    "import queue\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import redis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from core import support, share_param\n",
    "from insight_face.modules.tracking.custom_tracking import TrackingMultiCam\n",
    "from insight_face.utils.database import FaceRecognitionSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "  (prelu): PReLU(num_parameters=64)\n",
      "  (layer1): Sequential(\n",
      "    (0): IBasicBlock(\n",
      "      (bn1): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (prelu): PReLU(num_parameters=64)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): IBasicBlock(\n",
      "      (bn1): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (prelu): PReLU(num_parameters=64)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): IBasicBlock(\n",
      "      (bn1): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (prelu): PReLU(num_parameters=64)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): IBasicBlock(\n",
      "      (bn1): BatchNorm2d(64, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (prelu): PReLU(num_parameters=128)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(128, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): IBasicBlock(\n",
      "      (bn1): BatchNorm2d(128, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (prelu): PReLU(num_parameters=128)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(128, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): IBasicBlock(\n",
      "      (bn1): BatchNorm2d(128, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (prelu): PReLU(num_parameters=128)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(128, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): IBasicBlock(\n",
      "      (bn1): BatchNorm2d(128, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (prelu): PReLU(num_parameters=128)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(128, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): IBasicBlock(\n",
      "      (bn1): BatchNorm2d(128, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (prelu): PReLU(num_parameters=256)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): IBasicBlock(\n",
      "      (bn1): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (prelu): PReLU(num_parameters=256)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): IBasicBlock(\n",
      "      (bn1): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (prelu): PReLU(num_parameters=256)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): IBasicBlock(\n",
      "      (bn1): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (prelu): PReLU(num_parameters=256)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): IBasicBlock(\n",
      "      (bn1): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (prelu): PReLU(num_parameters=256)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (5): IBasicBlock(\n",
      "      (bn1): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (prelu): PReLU(num_parameters=256)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): IBasicBlock(\n",
      "      (bn1): BatchNorm2d(256, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (prelu): PReLU(num_parameters=512)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): IBasicBlock(\n",
      "      (bn1): BatchNorm2d(512, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (prelu): PReLU(num_parameters=512)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): IBasicBlock(\n",
      "      (bn1): BatchNorm2d(512, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "      (prelu): PReLU(num_parameters=512)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (bn2): BatchNorm2d(512, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      "  (dropout): Dropout2d(p=0.4, inplace=True)\n",
      "  (fc): Linear(in_features=25088, out_features=512, bias=True)\n",
      "  (features): BatchNorm1d(512, eps=2e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/khiemtv/.cache/torch/hub/ultralytics_yolov5_master\n",
      "Fusing layers... \n",
      "Model Summary: 224 layers, 7059304 parameters, 0 gradients, 16.3 GFLOPs\n",
      "Adding AutoShape... \n",
      "YOLOv5 ðŸš€ 2021-6-16 torch 1.8.1 CUDA:0 (GeForce RTX 2060, 5931.4375MB)\n",
      "\n",
      "Finish SDK initialization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mask', 'glasses', 'hand']\n"
     ]
    }
   ],
   "source": [
    "share_param.sdk_config = support.get_config_yaml(\"configs/sdk_config.yaml\")\n",
    "share_param.dev_config = support.get_config_yaml(\"configs/dev_config.yaml\")\n",
    "share_param.facerec_system = FaceRecognitionSystem(share_param.dev_config[\"DATA\"][\"photo_path\"], share_param.sdk_config )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-524ac2fbdeb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mbgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshare_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfacerec_system\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;31m# print(bboxes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# print(landmarks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/LINUXDATA/Source/object_tracking_v2/insight_face/sdk.py\u001b[0m in \u001b[0;36mdetect_faces\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \"\"\"\n\u001b[1;32m    174\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start faces detection.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Finish faces detection. Count of detected faces: {len(bboxes)}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandmarks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/LINUXDATA/Source/object_tracking_v2/insight_face/modules/detection/retinaface/model_class.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_input_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mraw_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mbboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlandms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_postprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/LINUXDATA/Source/object_tracking_v2/insight_face/modules/detection/retinaface/model_class.py\u001b[0m in \u001b[0;36m_predict_raw\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;31m# loc = loc.detach().cpu().numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# conf = conf.detach().cpu().numpy()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/openvino/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/LINUXDATA/Source/object_tracking_v2/insight_face/modules/detection/retinaface/dependencies/retinaface.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# FPN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mfpn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# SSH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/openvino/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/LINUXDATA/Source/object_tracking_v2/insight_face/modules/detection/retinaface/dependencies/net.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mup3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"nearest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0moutput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mup3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0moutput2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#CROP\n",
    "count = 0\n",
    "for root, dirs, files in os.walk(\"/mnt/LINUXDATA/Source/.data/face_attributes/mask02\", topdown=False):\n",
    "    # print(root)\n",
    "    for name in files:\n",
    "        count+=1\n",
    "        path = os.path.join(root, name)\n",
    "        bgr = cv2.imread(path)\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        bboxes, landmarks = share_param.facerec_system.sdk.detect_faces(rgb)\n",
    "        # print(bboxes)\n",
    "        # print(landmarks)\n",
    "        for bbox, landmark in zip(bboxes, landmarks):\n",
    "            faceW = bbox[2] - bbox[0]\n",
    "            faceH = bbox[3] - bbox[1]\n",
    "            expandLeft = max(0, bbox[0] - faceW/3)\n",
    "            expandTop = max(0, bbox[1] - faceH/3)\n",
    "            expandRight = min(bbox[2] + faceW/3, bgr.shape[1])\n",
    "            expandBottom = min(bbox[3] + faceH/3, bgr.shape[0])\n",
    "            faceCropExpand = bgr[int(expandTop):int(expandBottom), int(expandLeft):int(expandRight)].copy()\n",
    "            cv2.imwrite(f\"dataset/mask02/{count:04d}.jpg\", faceCropExpand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_not_blur(image: np.ndarray):\n",
    "    if image is None or image.size == 0:\n",
    "        return False, 0.0\n",
    "\n",
    "    # gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    imageresize = cv2.resize(image, (112,112))\n",
    "    real_notblur = cv2.Laplacian(imageresize, cv2.CV_64F).var()\n",
    "    standard_notblur = 300\n",
    "    threshnotblur = real_notblur/standard_notblur\n",
    "    print(threshnotblur)\n",
    "\n",
    "    if threshnotblur < 1.5:\n",
    "        return False, threshnotblur\n",
    "    else:\n",
    "        return True, threshnotblur\n",
    "    return False, threshnotblur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def illumination(image):\n",
    "    image = cv2.resize(image, (112,112))\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # length of R available  range  of  gray  intensities  excluding  5%  of  the darkest  and  brightest  pixel\n",
    "    sorted_gray = np.sort(gray.ravel())\n",
    "    l = len(sorted_gray)\n",
    "    cut_off_idx = l * 5 // 100\n",
    "    r = sorted_gray[l-cut_off_idx] - sorted_gray[cut_off_idx]\n",
    "    return np.round(r / 255, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check quality\n",
    "Blur and Straight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "count = 0\n",
    "for root, dirs, files in os.walk(\"dataset/bestphotos\", topdown=False):\n",
    "    # print(root)\n",
    "    for name in files:\n",
    "        count+=1\n",
    "        if count>100:\n",
    "            break\n",
    "        # print(name)\n",
    "        # if name != \"focus_02550.jpg\":\n",
    "        #     continue\n",
    "        path = os.path.join(root, name)\n",
    "        rgb = cv2.imread(path)\n",
    "        # rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "        bboxes, landmarks = share_param.facerec_system.sdk.detect_faces(rgb)\n",
    "        # print(bboxes)\n",
    "        # print(landmarks)\n",
    "        for bbox, landmark in zip(bboxes, landmarks):\n",
    "            faceSize = float((bbox[3]-bbox[1])*(bbox[2]-bbox[0]))\n",
    "            faceAlign = share_param.facerec_system.sdk.align_face(rgb, landmark)\n",
    "            faceCrop = rgb[int(bbox[1]):int(bbox[3]),\n",
    "                                    int(bbox[0]):int(bbox[2])]\n",
    "\n",
    "            if faceCrop.shape[0] < 50 or faceCrop.shape[1] < 50:\n",
    "                continue\n",
    "            isillumination, threshillumination = share_param.facerec_system.sdk.evaluter.check_illumination(faceCrop)\n",
    "            isNotBlur, threshnotblur = share_param.facerec_system.sdk.evaluter.check_not_blur(faceCrop)\n",
    "            isStraightFace = share_param.facerec_system.sdk.evaluter.check_straight_face(rgb, landmark)\n",
    "\n",
    "            # isNotBlur, threshnotblur = share_param.facerec_system.sdk.evaluter.check_not_blur(faceCrop)\n",
    "            # illuminate = share_param.facerec_system.sdk.evaluter.check_illumination(faceCrop)\n",
    "            # isNotBlur, threshnotblur = share_param.facerec_system.sdk.evaluter.detect_blur_fft(faceAlign, size=30)\n",
    "            # isStraightFace = share_param.facerec_system.sdk.evaluter.check_straight_face(rgb, landmark)\n",
    "            # print(\"Not blur:\", isNotBlur)\n",
    "            # print(\"Straight:\", isStraightFace)\n",
    "            cnts = landmark.reshape(5,2, order='F')\n",
    "            # for point in cnts:\n",
    "            #     cv2.drawMarker(rgb, tuple(point), color=(0,255,0), markerType=cv2.MARKER_CROSS, markerSize=8, thickness=1)\n",
    "            plt.figure()\n",
    "            plt.title(f\"{name} {threshillumination} {threshnotblur} {isStraightFace}\")\n",
    "            plt.imshow(faceCrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('datalab/images/20210507145554353919.jpg')\n",
    "# img = cv2.resize(img, (224,224))\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "minsize = 20 # minimum size of face\n",
    "threshold = [ 0.6, 0.7, 0.7 ]  # three steps's threshold\n",
    "factor = 0.709 # scale factor\n",
    "\n",
    "#Size Parameter\n",
    "lower_threshold = 100\n",
    "upper_threshold = 200\n",
    "\n",
    "bboxes, points = share_param.facerec_system.sdk.detect_faces(img)\n",
    "\n",
    "print(bboxes, points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def illumination(gray):\n",
    "    # length of R available  range  of  gray  intensities  excluding  5%  of  the darkest  and  brightest  pixel\n",
    "    sorted_gray = np.sort(gray.ravel())\n",
    "    l = len(sorted_gray)\n",
    "    cut_off_idx = l * 5 // 100\n",
    "    r = sorted_gray[l-cut_off_idx] - sorted_gray[cut_off_idx]\n",
    "    return np.round(r / 255, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illumination(gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contour(pts):\n",
    "    return np.array([[pts[i], pts[5 + i]] for i in  [0, 1, 4, 3]], np.int32).reshape((-1,1,2))\n",
    "def get_mask(image, contour):\n",
    "    mask = np.zeros(image.shape[0:2],dtype=\"uint8\")\n",
    "    cv2.drawContours(mask, [contour], -1, 255, -1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpness(img, landmark):\n",
    "    contour = get_contour(landmark)\n",
    "    mask = get_mask(img, contour) #1-channel mask\n",
    "    # plt.imshow(mask)\n",
    "    mask = np.stack((mask,)*3, axis=-1) #3-channel mask\n",
    "    mask[mask == 255] = 1 # convert 0 and 255 to 0 and 1\n",
    "    laplacian = cv2.Laplacian(img,cv2.CV_64F)\n",
    "    # print(laplacian)\n",
    "    # print(laplacian.var())\n",
    "    edges = laplacian[mask.astype(bool)]\n",
    "    return np.round(edges.var() / 255 , 2)\n",
    "\n",
    "plt.imshow(img)\n",
    "sharpness(img, points[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "\n",
    "def symmetry(img, landmark, bounding_box):\n",
    "    x1, y1, x2, y2 = int(min(bounding_box[0], min(landmark[:5]))), \\\n",
    "        int(min(bounding_box[1], min(landmark[5:]))), \\\n",
    "        int(max(bounding_box[2], max(landmark[:5]))), \\\n",
    "        int(max(bounding_box[3], max(landmark[5:])))\n",
    "    \n",
    "    landmark = np.array([[landmark[i], landmark[5 + i]] for i in  range(5)], np.int32).reshape((-1,1,2))\n",
    "    contour = landmark[:, 0] - [[x1, y1]]\n",
    "    \n",
    "    face = img[y1: y2, x1: x2].copy()\n",
    "    face_flip = cv2.flip(face, 1)\n",
    "    \n",
    "    fd_face, hog_face = hog(face, orientations=8, pixels_per_cell=(16, 16),\n",
    "                    cells_per_block=(1, 1), visualize=True, multichannel=True)\n",
    "    fd_flip, hog_flip = hog(face_flip, orientations=8, pixels_per_cell=(16, 16),\n",
    "                    cells_per_block=(1, 1), visualize=True, multichannel=True)\n",
    "    \n",
    "    \n",
    "    d = np.zeros(len(contour))\n",
    "    for i in range(len(d)):\n",
    "        d[i] = min(hog_face[contour[i, 1], contour[i, 0]], hog_flip[contour[i, 1], contour[i, 0]])\n",
    "    \n",
    "    return np.average(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symmetry(img, points[0], bboxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for root, dirs, files in os.walk(\"datalab/images/\", topdown=False):\n",
    "    # print(root)\n",
    "    for name in files:\n",
    "        path = os.path.join(root, name)\n",
    "        img = cv2.imread(path)\n",
    "        # img = cv2.resize(img, (224,224))\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        bboxes, points = share_param.facerec_system.sdk.detect_faces(img)\n",
    "        if len(bboxes) == 0:\n",
    "            continue\n",
    "        # print(bboxes)\n",
    "        # print(points)\n",
    "        # print(\"illumination\",illumination(gray))\n",
    "        # print(\"sharpness\",sharpness(img, points[0]))\n",
    "        # print(\"symmetry\", symmetry(img, points[0], bboxes[0]))\n",
    "\n",
    "        cv2.rectangle(img, (int(bboxes[0][0]), int(bboxes[0][1])), (int(bboxes[0][2]), int(bboxes[0][3])), (0, 255, 0), 2)\n",
    "        plt.figure()\n",
    "        strInfo = f\"{illumination(gray)} {sharpness(img, points[0])} {symmetry(img, points[0], bboxes[0])}\"\n",
    "        plt.title(strInfo)\n",
    "        plt.imshow(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpness 0.27\n",
    "symmetry 0.11464042663574218"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST CARD ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import json\n",
    "import threading\n",
    "import queue\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import redis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from core import support, share_param\n",
    "# from insight_face.modules.tracking.custom_tracking import TrackingMultiCam\n",
    "# from insight_face.utils.database import FaceRecognitionSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(image):\n",
    "    h= image.shape[0]\n",
    "    w= image.shape[1]\n",
    "\n",
    "    minsize = int(min(w,h)*2/3)\n",
    "\n",
    "    # print(w,h)\n",
    "    # print(minsize)\n",
    "\n",
    "    xstart = (w - minsize)//2\n",
    "    ystart = (h - minsize)//2\n",
    "    frame = image[ystart:ystart+minsize, xstart: xstart + minsize]\n",
    "    return frame\n",
    "\n",
    "def check_illumination(self, image):\n",
    "    if image is None or image.size == 0:\n",
    "        return False, 0.0\n",
    "    image = cv2.resize(image, (112,112))\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # length of R available  range  of  gray  intensities  excluding  5%  of  the darkest  and  brightest  pixel\n",
    "    sorted_gray = np.sort(gray.ravel())\n",
    "    l = len(sorted_gray)\n",
    "    cut_off_idx = l * 5 // 100\n",
    "    r = sorted_gray[l-cut_off_idx] - sorted_gray[cut_off_idx]\n",
    "    illuminate = np.round(r / 255, 2)\n",
    "\n",
    "    if illuminate < self.illumination_threshold:\n",
    "        return False, illuminate\n",
    "\n",
    "    return True, illuminate\n",
    "\n",
    "def check_illuminationhsv(self, image):\n",
    "    if image is None or image.size == 0:\n",
    "        return False, 0.0\n",
    "    image = cv2.resize(image, (112,112))\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    v = hsv[:,:,2]\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(v.ravel(),256,[0,256])\n",
    "    plt.show()\n",
    "\n",
    "    # sorted_gray = np.sort(v.ravel())\n",
    "    # l = len(sorted_gray)\n",
    "    # cut_off_idx = l * 5 // 100\n",
    "    # r = sorted_gray[l-cut_off_idx] - sorted_gray[cut_off_idx]\n",
    "    # illuminate = np.round(r / 255, 2)\n",
    "\n",
    "    illuminate = v.std()\n",
    "\n",
    "    if illuminate < self.illumination_threshold:\n",
    "        return False, illuminate\n",
    "\n",
    "    return True, illuminate   \n",
    "    # print(v)\n",
    "    # plt.figure()\n",
    "    # plt.hist(v.ravel(), bins=256, range=(0.0, 1.0), fc='k', ec='k') #calculating histogram\n",
    "\n",
    "    # print(v.shape)\n",
    "\n",
    "    # plt.figure()\n",
    "    # plt.title(\"brightvalue\")\n",
    "    # plt.imshow(v, cmap='gray')\n",
    "\n",
    "    return True , 10\n",
    "\n",
    "\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # length of R available  range  of  gray  intensities  excluding  5%  of  the darkest  and  brightest  pixel\n",
    "    sorted_gray = np.sort(gray.ravel())\n",
    "    l = len(sorted_gray)\n",
    "    cut_off_idx = l * 5 // 100\n",
    "    r = sorted_gray[l-cut_off_idx] - sorted_gray[cut_off_idx]\n",
    "    illuminate = np.round(r / 255, 2)\n",
    "\n",
    "    if illuminate < self.illumination_threshold:\n",
    "        return False, illuminate\n",
    "\n",
    "    return True, illuminate        \n",
    "\n",
    "def check_not_blur(self, image: np.ndarray):\n",
    "    if image is None or image.size == 0:\n",
    "        return False, 0.0\n",
    "\n",
    "    image = cv2.resize(image, (112,112))\n",
    "\n",
    "    real_notblur = cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "    standard_notblur = 300\n",
    "\n",
    "    threshnotblur = real_notblur/standard_notblur\n",
    "    # print(threshnotblur)\n",
    "\n",
    "    if threshnotblur < self.blur_threshold:\n",
    "        return False, threshnotblur\n",
    "    else:\n",
    "        return True, threshnotblur\n",
    "    return False, threshnotblur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "class Param:\n",
    "    illumination_threshold = 0.8\n",
    "    blur_threshold = 0.8\n",
    "\n",
    "for root, dirs, files in os.walk(\"/mnt/LINUXDATA/Source/data/datacmt\", topdown=False):\n",
    "    if count>100:\n",
    "            break\n",
    "    for name in files:\n",
    "        count+=1\n",
    "        \n",
    "        path = os.path.join(root, name)\n",
    "        bgr = cv2.imread(path)\n",
    "        bgr = prepare(bgr)\n",
    "        # print(bgr.shape)\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        isNotBlur, threshnotblur = check_not_blur(Param, rgb)\n",
    "        illuminate, threshillumination = check_illuminationhsv(Param, rgb)\n",
    "        strR = \"{:03.3f} {:03.3f}\".format(threshnotblur, threshillumination)\n",
    "        pt1 = [10, 20]\n",
    "        fontFace = cv2.FONT_HERSHEY_COMPLEX_SMALL\n",
    "        fontScale = 0.5\n",
    "        thickness = 1\n",
    "        retval, baseLine = cv2.getTextSize(strR,fontFace=fontFace,fontScale=fontScale, thickness=thickness)\n",
    "        # Calculate the coordinates of the rectangular frame covering the text\n",
    "        topleft = (pt1[0], pt1[1] - retval[1])\n",
    "        bottomright = (topleft[0] + retval[0], topleft[1] + retval[1])\n",
    "        cv2.rectangle(rgb, (topleft[0], topleft[1] - baseLine), bottomright,thickness=-1, color=(0, 255, 0))\n",
    "        # Draw text\n",
    "        cv2.putText(rgb, strR, (pt1[0], pt1[1]-baseLine), fontScale=fontScale,fontFace=fontFace, thickness=thickness, color=(0,0,0))\n",
    "\n",
    "        # cv2.rectangle(bgr, (10, 10-textSize[1]), (10 + int(textSize[0][0]), 10 + int(textSize[0][1])), (0,0,0), -1)\n",
    "    \n",
    "        # cv2.putText(bgr, \"{:03.3f} {:03.3f}\".format(threshnotblur, threshillumination), (10,10-textSize[1]), cv2.FONT_HERSHEY_COMPLEX_SMALL, 0.5, (0,255,0), 1)\n",
    "        plt.figure()\n",
    "        plt.title(strR)\n",
    "        plt.imshow(rgb)\n",
    "        savename = \"R\"+name\n",
    "        cv2.imwrite(os.path.join(\"/mnt/LINUXDATA/Source/data/\", savename),cv2.cvtColor(rgb,cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CONVERT JIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import json\n",
    "import threading\n",
    "import queue\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import redis\n",
    "import matplotlib.pyplot as plt\n",
    "from core import support\n",
    "\n",
    "from insight_face.modules.detection.retinaface.model_class import RetinaFace\n",
    "\n",
    "# from core import support, share_param\n",
    "# from insight_face.modules.tracking.custom_tracking import TrackingMultiCam\n",
    "# from insight_face.utils.database import FaceRecognitionSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdk_config = support.get_config_yaml(\"configs/sdk_config.yaml\")\n",
    "# share_param.dev_config = support.get_config_yaml(\"configs/dev_config.yaml\")\n",
    "# share_param.facerec_system = FaceRecognitionSystem(share_param.dev_config[\"DATA\"][\"photo_path\"], share_param.sdk_config )\n",
    "\n",
    "detector = RetinaFace(sdk_config[\"detector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgr = cv2.imread(\"dataset/bestphotos/113900549523G.jpg\")\n",
    "bgr = cv2.resize(bgr, (336,336))\n",
    "rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "# preTime = time.time()\n",
    "image = detector._preprocess(rgb)\n",
    "detector.model_input_shape = image.shape\n",
    "\n",
    "img = image.transpose((2, 0, 1))\n",
    "img = torch.from_numpy(img).unsqueeze(0)\n",
    "img = img.to(detector.device)\n",
    "# pred = detector.model(img)\n",
    "\n",
    "detector.model = torch.jit.trace(detector.model,img)\n",
    "# print(pred)\n",
    "\n",
    "# raw_pred = detector._predict_raw(image)\n",
    "\n",
    "# bgr = cv2.resize(bgr, (336,336))\n",
    "# bgr = cv2.resize(bgr, (336,336))\n",
    "# rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "# detector.predict(image)\n",
    "# traced_predict = torch.jit.trace(detector._predict_raw, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.model.half()\n",
    "for filename in glob.glob(\"/mnt/LINUXDATA/Source/.data/face_attributes/01000-20210525T030241Z-001/01000/*.png\"):\n",
    "    bgr = cv2.imread(filename)\n",
    "    bgr = cv2.resize(bgr, (336,336))\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    image = detector._preprocess(rgb)\n",
    "    detector.model_input_shape = image.shape\n",
    "    preTime = time.time()\n",
    "    # image.astype(np.float16)\n",
    "    raw_pred = detector._predict_raw(image)\n",
    "    print(\"Detect:\", time.time()-preTime)\n",
    "\n",
    "    bboxes, landms = detector._postprocess(raw_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EXPORT OPENVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EXPORT RETINA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from insight_face.modules.detection.retinaface.model_class import RetinaFace\n",
    "import cv2\n",
    "from core import support\n",
    "\n",
    "config = {\n",
    "    \"image_size\": 336,                   # image size input to detector. Default=224\n",
    "    \"nms_threshold\": 0.5,                   # nms\n",
    "    \"conf_threshold\": 0.8,               # confidence\n",
    "    \"minface\": 50,                       # min face size. Default=50\n",
    "    \"device\": \"cpu\",                      # cpu, cuda\n",
    "    \"architecture\": \"mnet1\"               # res50 (ResNet 50), mnet1 (mobilenet1)\n",
    "}\n",
    "\n",
    "detector = RetinaFace(config)\n",
    "\n",
    "dummy_input = torch.zeros((1,3,config[\"image_size\"],config[\"image_size\"]))\n",
    "torch.onnx.export(detector.model, dummy_input, f'weights/{sdk_config[\"detector\"][\"architecture\"]}.onnx', opset_version=11)\n",
    "\n",
    "! python ~/intel/openvino_2021.3.394/deployment_tools/model_optimizer/mo_onnx.py --input_model=weights/mnet1.onnx --output_dir=weights \n",
    "# --data_type=FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from insight_face.modules.detection.retinaface.model_class import RetinaFace\n",
    "import cv2\n",
    "from core import support\n",
    "\n",
    "config = {\n",
    "    \"image_size\": 336,                   # image size input to detector. Default=224\n",
    "    \"nms_threshold\": 0.5,                   # nms\n",
    "    \"conf_threshold\": 0.8,               # confidence\n",
    "    \"minface\": 50,                       # min face size. Default=50\n",
    "    \"device\": \"cpu\",                      # cpu, cuda\n",
    "    \"architecture\": \"res50\"               # res50 (ResNet 50), mnet1 (mobilenet1)\n",
    "}\n",
    "\n",
    "detector = RetinaFace(config)\n",
    "\n",
    "dummy_input = torch.zeros((1,3,config[\"image_size\"],config[\"image_size\"]))\n",
    "torch.onnx.export(detector.model, dummy_input, f'weights/{config[\"architecture\"]}.onnx', opset_version=11)\n",
    "\n",
    "! python ~/intel/openvino_2021.3.394/deployment_tools/model_optimizer/mo_onnx.py --input_model=weights/res50.onnx --output_dir=weights \n",
    "# --data_type=FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### EXPORT FACE EMMBED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from insight_face.modules.recognition.insightface.insightface import InsightFaceEmbedder\n",
    "import cv2\n",
    "from core import support\n",
    "\n",
    "config = {\n",
    "    \"image_size\": 112,                   # input size of emmbeder \n",
    "    \"descriptor_size\": 512,\n",
    "    \"device\": \"cpu\",                      # cpu, cuda\n",
    "    \"architecture\": \"iresnet34\"           # iresnet100, iresnet50, iresnet34\n",
    "}\n",
    "\n",
    "face_embedder = InsightFaceEmbedder(config)\n",
    "\n",
    "dummy_input = torch.zeros((1,3,config[\"image_size\"],config[\"image_size\"]))\n",
    "torch.onnx.export(face_embedder.embedder, dummy_input, f'weights/{config[\"architecture\"]}.onnx', opset_version=11)\n",
    "\n",
    "! python ~/anaconda3/envs/openvino/lib/python3.7/site-packages/mo_onnx.py --input_model=weights/iresnet34.onnx --output_dir=weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from insight_face.modules.recognition.insightface.insightface import InsightFaceEmbedder\n",
    "import cv2\n",
    "from core import support\n",
    "\n",
    "config = {\n",
    "    \"image_size\": 112,                   # input size of emmbeder \n",
    "    \"descriptor_size\": 512,\n",
    "    \"device\": \"cpu\",                      # cpu, cuda\n",
    "    \"architecture\": \"iresnet50\"           # iresnet100, iresnet50, iresnet34\n",
    "}\n",
    "\n",
    "face_embedder = InsightFaceEmbedder(config)\n",
    "\n",
    "dummy_input = torch.zeros((1,3,config[\"image_size\"],config[\"image_size\"]))\n",
    "torch.onnx.export(face_embedder.embedder, dummy_input, f'weights/{config[\"architecture\"]}.onnx', opset_version=11)\n",
    "\n",
    "! python ~/anaconda3/envs/openvino/lib/python3.7/site-packages/mo_onnx.py --input_model=weights/iresnet50.onnx --output_dir=weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from insight_face.modules.recognition.insightface.insightface import InsightFaceEmbedder\n",
    "import cv2\n",
    "from core import support\n",
    "\n",
    "config = {\n",
    "    \"image_size\": 112,                   # input size of emmbeder \n",
    "    \"descriptor_size\": 512,\n",
    "    \"device\": \"cpu\",                      # cpu, cuda\n",
    "    \"architecture\": \"iresnet100\"           # iresnet100, iresnet50, iresnet34\n",
    "}\n",
    "\n",
    "face_embedder = InsightFaceEmbedder(config)\n",
    "\n",
    "dummy_input = torch.zeros((1,3,config[\"image_size\"],config[\"image_size\"]))\n",
    "torch.onnx.export(face_embedder.embedder, dummy_input, f'weights/{config[\"architecture\"]}.onnx', opset_version=11)\n",
    "\n",
    "! python ~/anaconda3/envs/openvino/lib/python3.7/site-packages/mo_onnx.py --input_model=weights/iresnet100.onnx --output_dir=weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# os.environ['LD_LIBRARY_PATH'] = \"/home/khiemtv/anaconda3/envs/openvino/lib\"\n",
    "# print(os.environ['LD_LIBRARY_PATH'])\n",
    "from openvino.inference_engine import IECore\n",
    "\n",
    "ie = IECore()\n",
    "net = ie.read_network(\"weights/iresnet34.xml\", \"weights/iresnet34.bin\")\n",
    "exec_net = ie.load_network(network=net, device_name=\"CPU\")\n",
    "image = np.zeros((1,3,112,112))\n",
    "# data[input_name] = images\n",
    "# print(face_embedder.embedder(dummy_input))\n",
    "\n",
    "ten_res = face_embedder.embedder(dummy_input).detach().numpy()\n",
    "res = exec_net.infer(inputs={\"input.1\": image})\n",
    "# print(res)\n",
    "# print(abs((ten_res - res['475'])/.sum())/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2f1f12eb66b03e8d9695162c79e98253b80e970c6ca8f0e78ea223164db03af3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('openvino': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "2f1f12eb66b03e8d9695162c79e98253b80e970c6ca8f0e78ea223164db03af3"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}