{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd02f1f12eb66b03e8d9695162c79e98253b80e970c6ca8f0e78ea223164db03af3",
   "display_name": "Python 3.7.10 64-bit ('openvino': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "2f1f12eb66b03e8d9695162c79e98253b80e970c6ca8f0e78ea223164db03af3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import json\n",
    "import threading\n",
    "import queue\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import redis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from core import support, share_param\n",
    "from insight_face.modules.tracking.custom_tracking import TrackingMultiCam\n",
    "from insight_face.utils.database import FaceRecognitionSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_param.sdk_config = support.get_config_yaml(\"configs/sdk_config.yaml\")\n",
    "share_param.dev_config = support.get_config_yaml(\"configs/dev_config.yaml\")\n",
    "share_param.facerec_system = FaceRecognitionSystem(share_param.dev_config[\"DATA\"][\"photo_path\"], share_param.sdk_config )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_not_blur(image: np.ndarray):\n",
    "    if image is None or image.size == 0:\n",
    "        return False, 0.0\n",
    "\n",
    "    # gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    imageresize = cv2.resize(image, (112,112))\n",
    "    real_notblur = cv2.Laplacian(imageresize, cv2.CV_64F).var()\n",
    "    standard_notblur = 300\n",
    "    threshnotblur = real_notblur/standard_notblur\n",
    "    print(threshnotblur)\n",
    "\n",
    "    if threshnotblur < 1.5:\n",
    "        return False, threshnotblur\n",
    "    else:\n",
    "        return True, threshnotblur\n",
    "    return False, threshnotblur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def illumination(image):\n",
    "    image = cv2.resize(image, (112,112))\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # length of R available  range  of  gray  intensities  excluding  5%  of  the darkest  and  brightest  pixel\n",
    "    sorted_gray = np.sort(gray.ravel())\n",
    "    l = len(sorted_gray)\n",
    "    cut_off_idx = l * 5 // 100\n",
    "    r = sorted_gray[l-cut_off_idx] - sorted_gray[cut_off_idx]\n",
    "    return np.round(r / 255, 2)"
   ]
  },
  {
   "source": [
    "### Check quality\n",
    "Blur and Straight"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "count = 0\n",
    "for root, dirs, files in os.walk(\"dataset/bestphotos\", topdown=False):\n",
    "    # print(root)\n",
    "    for name in files:\n",
    "        count+=1\n",
    "        if count>100:\n",
    "            break\n",
    "        # print(name)\n",
    "        # if name != \"focus_02550.jpg\":\n",
    "        #     continue\n",
    "        path = os.path.join(root, name)\n",
    "        rgb = cv2.imread(path)\n",
    "        # rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "        bboxes, landmarks = share_param.facerec_system.sdk.detect_faces(rgb)\n",
    "        # print(bboxes)\n",
    "        # print(landmarks)\n",
    "        for bbox, landmark in zip(bboxes, landmarks):\n",
    "            faceSize = float((bbox[3]-bbox[1])*(bbox[2]-bbox[0]))\n",
    "            faceAlign = share_param.facerec_system.sdk.align_face(rgb, landmark)\n",
    "            faceCrop = rgb[int(bbox[1]):int(bbox[3]),\n",
    "                                    int(bbox[0]):int(bbox[2])]\n",
    "\n",
    "            if faceCrop.shape[0] < 50 or faceCrop.shape[1] < 50:\n",
    "                continue\n",
    "            isillumination, threshillumination = share_param.facerec_system.sdk.evaluter.check_illumination(faceCrop)\n",
    "            isNotBlur, threshnotblur = share_param.facerec_system.sdk.evaluter.check_not_blur(faceCrop)\n",
    "            isStraightFace = share_param.facerec_system.sdk.evaluter.check_straight_face(rgb, landmark)\n",
    "\n",
    "            # isNotBlur, threshnotblur = share_param.facerec_system.sdk.evaluter.check_not_blur(faceCrop)\n",
    "            # illuminate = share_param.facerec_system.sdk.evaluter.check_illumination(faceCrop)\n",
    "            # isNotBlur, threshnotblur = share_param.facerec_system.sdk.evaluter.detect_blur_fft(faceAlign, size=30)\n",
    "            # isStraightFace = share_param.facerec_system.sdk.evaluter.check_straight_face(rgb, landmark)\n",
    "            # print(\"Not blur:\", isNotBlur)\n",
    "            # print(\"Straight:\", isStraightFace)\n",
    "            cnts = landmark.reshape(5,2, order='F')\n",
    "            # for point in cnts:\n",
    "            #     cv2.drawMarker(rgb, tuple(point), color=(0,255,0), markerType=cv2.MARKER_CROSS, markerSize=8, thickness=1)\n",
    "            plt.figure()\n",
    "            plt.title(f\"{name} {threshillumination} {threshnotblur} {isStraightFace}\")\n",
    "            plt.imshow(faceCrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('datalab/images/20210507145554353919.jpg')\n",
    "# img = cv2.resize(img, (224,224))\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "minsize = 20 # minimum size of face\n",
    "threshold = [ 0.6, 0.7, 0.7 ]  # three steps's threshold\n",
    "factor = 0.709 # scale factor\n",
    "\n",
    "#Size Parameter\n",
    "lower_threshold = 100\n",
    "upper_threshold = 200\n",
    "\n",
    "bboxes, points = share_param.facerec_system.sdk.detect_faces(img)\n",
    "\n",
    "print(bboxes, points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def illumination(gray):\n",
    "    # length of R available  range  of  gray  intensities  excluding  5%  of  the darkest  and  brightest  pixel\n",
    "    sorted_gray = np.sort(gray.ravel())\n",
    "    l = len(sorted_gray)\n",
    "    cut_off_idx = l * 5 // 100\n",
    "    r = sorted_gray[l-cut_off_idx] - sorted_gray[cut_off_idx]\n",
    "    return np.round(r / 255, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illumination(gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contour(pts):\n",
    "    return np.array([[pts[i], pts[5 + i]] for i in  [0, 1, 4, 3]], np.int32).reshape((-1,1,2))\n",
    "def get_mask(image, contour):\n",
    "    mask = np.zeros(image.shape[0:2],dtype=\"uint8\")\n",
    "    cv2.drawContours(mask, [contour], -1, 255, -1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpness(img, landmark):\n",
    "    contour = get_contour(landmark)\n",
    "    mask = get_mask(img, contour) #1-channel mask\n",
    "    # plt.imshow(mask)\n",
    "    mask = np.stack((mask,)*3, axis=-1) #3-channel mask\n",
    "    mask[mask == 255] = 1 # convert 0 and 255 to 0 and 1\n",
    "    laplacian = cv2.Laplacian(img,cv2.CV_64F)\n",
    "    # print(laplacian)\n",
    "    # print(laplacian.var())\n",
    "    edges = laplacian[mask.astype(bool)]\n",
    "    return np.round(edges.var() / 255 , 2)\n",
    "\n",
    "plt.imshow(img)\n",
    "sharpness(img, points[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "\n",
    "def symmetry(img, landmark, bounding_box):\n",
    "    x1, y1, x2, y2 = int(min(bounding_box[0], min(landmark[:5]))), \\\n",
    "        int(min(bounding_box[1], min(landmark[5:]))), \\\n",
    "        int(max(bounding_box[2], max(landmark[:5]))), \\\n",
    "        int(max(bounding_box[3], max(landmark[5:])))\n",
    "    \n",
    "    landmark = np.array([[landmark[i], landmark[5 + i]] for i in  range(5)], np.int32).reshape((-1,1,2))\n",
    "    contour = landmark[:, 0] - [[x1, y1]]\n",
    "    \n",
    "    face = img[y1: y2, x1: x2].copy()\n",
    "    face_flip = cv2.flip(face, 1)\n",
    "    \n",
    "    fd_face, hog_face = hog(face, orientations=8, pixels_per_cell=(16, 16),\n",
    "                    cells_per_block=(1, 1), visualize=True, multichannel=True)\n",
    "    fd_flip, hog_flip = hog(face_flip, orientations=8, pixels_per_cell=(16, 16),\n",
    "                    cells_per_block=(1, 1), visualize=True, multichannel=True)\n",
    "    \n",
    "    \n",
    "    d = np.zeros(len(contour))\n",
    "    for i in range(len(d)):\n",
    "        d[i] = min(hog_face[contour[i, 1], contour[i, 0]], hog_flip[contour[i, 1], contour[i, 0]])\n",
    "    \n",
    "    return np.average(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symmetry(img, points[0], bboxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for root, dirs, files in os.walk(\"datalab/images/\", topdown=False):\n",
    "    # print(root)\n",
    "    for name in files:\n",
    "        path = os.path.join(root, name)\n",
    "        img = cv2.imread(path)\n",
    "        # img = cv2.resize(img, (224,224))\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        bboxes, points = share_param.facerec_system.sdk.detect_faces(img)\n",
    "        if len(bboxes) == 0:\n",
    "            continue\n",
    "        # print(bboxes)\n",
    "        # print(points)\n",
    "        # print(\"illumination\",illumination(gray))\n",
    "        # print(\"sharpness\",sharpness(img, points[0]))\n",
    "        # print(\"symmetry\", symmetry(img, points[0], bboxes[0]))\n",
    "\n",
    "        cv2.rectangle(img, (int(bboxes[0][0]), int(bboxes[0][1])), (int(bboxes[0][2]), int(bboxes[0][3])), (0, 255, 0), 2)\n",
    "        plt.figure()\n",
    "        strInfo = f\"{illumination(gray)} {sharpness(img, points[0])} {symmetry(img, points[0], bboxes[0])}\"\n",
    "        plt.title(strInfo)\n",
    "        plt.imshow(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpness 0.27\n",
    "symmetry 0.11464042663574218"
   ]
  },
  {
   "source": [
    "### TEST CARD ID"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import json\n",
    "import threading\n",
    "import queue\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import redis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from core import support, share_param\n",
    "# from insight_face.modules.tracking.custom_tracking import TrackingMultiCam\n",
    "# from insight_face.utils.database import FaceRecognitionSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(image):\n",
    "    h= image.shape[0]\n",
    "    w= image.shape[1]\n",
    "\n",
    "    minsize = int(min(w,h)*2/3)\n",
    "\n",
    "    # print(w,h)\n",
    "    # print(minsize)\n",
    "\n",
    "    xstart = (w - minsize)//2\n",
    "    ystart = (h - minsize)//2\n",
    "    frame = image[ystart:ystart+minsize, xstart: xstart + minsize]\n",
    "    return frame\n",
    "\n",
    "def check_illumination(self, image):\n",
    "    if image is None or image.size == 0:\n",
    "        return False, 0.0\n",
    "    image = cv2.resize(image, (112,112))\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # length of R available  range  of  gray  intensities  excluding  5%  of  the darkest  and  brightest  pixel\n",
    "    sorted_gray = np.sort(gray.ravel())\n",
    "    l = len(sorted_gray)\n",
    "    cut_off_idx = l * 5 // 100\n",
    "    r = sorted_gray[l-cut_off_idx] - sorted_gray[cut_off_idx]\n",
    "    illuminate = np.round(r / 255, 2)\n",
    "\n",
    "    if illuminate < self.illumination_threshold:\n",
    "        return False, illuminate\n",
    "\n",
    "    return True, illuminate\n",
    "\n",
    "def check_illuminationhsv(self, image):\n",
    "    if image is None or image.size == 0:\n",
    "        return False, 0.0\n",
    "    image = cv2.resize(image, (112,112))\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    v = hsv[:,:,2]\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(v.ravel(),256,[0,256])\n",
    "    plt.show()\n",
    "\n",
    "    # sorted_gray = np.sort(v.ravel())\n",
    "    # l = len(sorted_gray)\n",
    "    # cut_off_idx = l * 5 // 100\n",
    "    # r = sorted_gray[l-cut_off_idx] - sorted_gray[cut_off_idx]\n",
    "    # illuminate = np.round(r / 255, 2)\n",
    "\n",
    "    illuminate = v.std()\n",
    "\n",
    "    if illuminate < self.illumination_threshold:\n",
    "        return False, illuminate\n",
    "\n",
    "    return True, illuminate   \n",
    "    # print(v)\n",
    "    # plt.figure()\n",
    "    # plt.hist(v.ravel(), bins=256, range=(0.0, 1.0), fc='k', ec='k') #calculating histogram\n",
    "\n",
    "    # print(v.shape)\n",
    "\n",
    "    # plt.figure()\n",
    "    # plt.title(\"brightvalue\")\n",
    "    # plt.imshow(v, cmap='gray')\n",
    "\n",
    "    return True , 10\n",
    "\n",
    "\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # length of R available  range  of  gray  intensities  excluding  5%  of  the darkest  and  brightest  pixel\n",
    "    sorted_gray = np.sort(gray.ravel())\n",
    "    l = len(sorted_gray)\n",
    "    cut_off_idx = l * 5 // 100\n",
    "    r = sorted_gray[l-cut_off_idx] - sorted_gray[cut_off_idx]\n",
    "    illuminate = np.round(r / 255, 2)\n",
    "\n",
    "    if illuminate < self.illumination_threshold:\n",
    "        return False, illuminate\n",
    "\n",
    "    return True, illuminate        \n",
    "\n",
    "def check_not_blur(self, image: np.ndarray):\n",
    "    if image is None or image.size == 0:\n",
    "        return False, 0.0\n",
    "\n",
    "    image = cv2.resize(image, (112,112))\n",
    "\n",
    "    real_notblur = cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "    standard_notblur = 300\n",
    "\n",
    "    threshnotblur = real_notblur/standard_notblur\n",
    "    # print(threshnotblur)\n",
    "\n",
    "    if threshnotblur < self.blur_threshold:\n",
    "        return False, threshnotblur\n",
    "    else:\n",
    "        return True, threshnotblur\n",
    "    return False, threshnotblur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "class Param:\n",
    "    illumination_threshold = 0.8\n",
    "    blur_threshold = 0.8\n",
    "\n",
    "for root, dirs, files in os.walk(\"/mnt/LINUXDATA/Source/data/datacmt\", topdown=False):\n",
    "    if count>100:\n",
    "            break\n",
    "    for name in files:\n",
    "        count+=1\n",
    "        \n",
    "        path = os.path.join(root, name)\n",
    "        bgr = cv2.imread(path)\n",
    "        bgr = prepare(bgr)\n",
    "        # print(bgr.shape)\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        isNotBlur, threshnotblur = check_not_blur(Param, rgb)\n",
    "        illuminate, threshillumination = check_illuminationhsv(Param, rgb)\n",
    "        strR = \"{:03.3f} {:03.3f}\".format(threshnotblur, threshillumination)\n",
    "        pt1 = [10, 20]\n",
    "        fontFace = cv2.FONT_HERSHEY_COMPLEX_SMALL\n",
    "        fontScale = 0.5\n",
    "        thickness = 1\n",
    "        retval, baseLine = cv2.getTextSize(strR,fontFace=fontFace,fontScale=fontScale, thickness=thickness)\n",
    "        # Calculate the coordinates of the rectangular frame covering the text\n",
    "        topleft = (pt1[0], pt1[1] - retval[1])\n",
    "        bottomright = (topleft[0] + retval[0], topleft[1] + retval[1])\n",
    "        cv2.rectangle(rgb, (topleft[0], topleft[1] - baseLine), bottomright,thickness=-1, color=(0, 255, 0))\n",
    "        # Draw text\n",
    "        cv2.putText(rgb, strR, (pt1[0], pt1[1]-baseLine), fontScale=fontScale,fontFace=fontFace, thickness=thickness, color=(0,0,0))\n",
    "\n",
    "        # cv2.rectangle(bgr, (10, 10-textSize[1]), (10 + int(textSize[0][0]), 10 + int(textSize[0][1])), (0,0,0), -1)\n",
    "    \n",
    "        # cv2.putText(bgr, \"{:03.3f} {:03.3f}\".format(threshnotblur, threshillumination), (10,10-textSize[1]), cv2.FONT_HERSHEY_COMPLEX_SMALL, 0.5, (0,255,0), 1)\n",
    "        plt.figure()\n",
    "        plt.title(strR)\n",
    "        plt.imshow(rgb)\n",
    "        savename = \"R\"+name\n",
    "        cv2.imwrite(os.path.join(\"/mnt/LINUXDATA/Source/data/\", savename),cv2.cvtColor(rgb,cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "source": [
    "#### CONVERT JIT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import json\n",
    "import threading\n",
    "import queue\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import redis\n",
    "import matplotlib.pyplot as plt\n",
    "from core import support\n",
    "\n",
    "from insight_face.modules.detection.retinaface.model_class import RetinaFace\n",
    "\n",
    "# from core import support, share_param\n",
    "# from insight_face.modules.tracking.custom_tracking import TrackingMultiCam\n",
    "# from insight_face.utils.database import FaceRecognitionSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdk_config = support.get_config_yaml(\"configs/sdk_config.yaml\")\n",
    "# share_param.dev_config = support.get_config_yaml(\"configs/dev_config.yaml\")\n",
    "# share_param.facerec_system = FaceRecognitionSystem(share_param.dev_config[\"DATA\"][\"photo_path\"], share_param.sdk_config )\n",
    "\n",
    "detector = RetinaFace(sdk_config[\"detector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgr = cv2.imread(\"dataset/bestphotos/113900549523G.jpg\")\n",
    "bgr = cv2.resize(bgr, (336,336))\n",
    "rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "# preTime = time.time()\n",
    "image = detector._preprocess(rgb)\n",
    "detector.model_input_shape = image.shape\n",
    "\n",
    "img = image.transpose((2, 0, 1))\n",
    "img = torch.from_numpy(img).unsqueeze(0)\n",
    "img = img.to(detector.device)\n",
    "# pred = detector.model(img)\n",
    "\n",
    "detector.model = torch.jit.trace(detector.model,img)\n",
    "# print(pred)\n",
    "\n",
    "# raw_pred = detector._predict_raw(image)\n",
    "\n",
    "# bgr = cv2.resize(bgr, (336,336))\n",
    "# bgr = cv2.resize(bgr, (336,336))\n",
    "# rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "# detector.predict(image)\n",
    "# traced_predict = torch.jit.trace(detector._predict_raw, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.model.half()\n",
    "for filename in glob.glob(\"/mnt/LINUXDATA/Source/.data/face_attributes/01000-20210525T030241Z-001/01000/*.png\"):\n",
    "    bgr = cv2.imread(filename)\n",
    "    bgr = cv2.resize(bgr, (336,336))\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    image = detector._preprocess(rgb)\n",
    "    detector.model_input_shape = image.shape\n",
    "    preTime = time.time()\n",
    "    # image.astype(np.float16)\n",
    "    raw_pred = detector._predict_raw(image)\n",
    "    print(\"Detect:\", time.time()-preTime)\n",
    "\n",
    "    bboxes, landms = detector._postprocess(raw_pred)\n"
   ]
  },
  {
   "source": [
    "#### EXPORT OPENVINO"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "##### EXPORT RETINA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/mnt/LINUXDATA/Source/object_tracking_v2/weights/mnet1.onnx\n",
      "\t- Path for generated IR: \t/mnt/LINUXDATA/Source/object_tracking_v2/weights\n",
      "\t- IR output name: \tmnet1\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tNone\n",
      "\t- Reverse input channels: \tFalse\n",
      "ONNX specific parameters:\n",
      "[ WARNING ] Failed to import Inference Engine Python API in: /home/khiemtv/intel/openvino_2021.3.394/python/python3.7\n",
      "[ WARNING ] libinference_engine.so: cannot open shared object file: No such file or directory\n",
      "\t- Inference Engine found in: \t/home/khiemtv/intel/openvino_2021.3.394/python/python3.7/openvino\n",
      "Inference Engine version: \t2.1.2021.3.0-2787-60059f2c755-releases/2021/3\n",
      "Model Optimizer version: \t    2021.3.0-2787-60059f2c755-releases/2021/3\n",
      "[ WARNING ]  \n",
      "Detected not satisfied dependencies:\n",
      "\ttest-generator: not installed, required: == 0.1.1\n",
      "\trequests: not installed, required: >= 2.20.0\n",
      "\n",
      "Please install required versions of components or use install_prerequisites script\n",
      "/home/khiemtv/intel/openvino_2021.3.394/deployment_tools/model_optimizer/install_prerequisites/install_prerequisites_onnx.sh\n",
      "Note that install_prerequisites scripts may install additional components.\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /mnt/LINUXDATA/Source/object_tracking_v2/weights/mnet1.xml\n",
      "[ SUCCESS ] BIN file: /mnt/LINUXDATA/Source/object_tracking_v2/weights/mnet1.bin\n",
      "[ SUCCESS ] Total execution time: 11.64 seconds. \n",
      "[ SUCCESS ] Memory consumed: 3066 MB. \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from insight_face.modules.detection.retinaface.model_class import RetinaFace\n",
    "import cv2\n",
    "from core import support\n",
    "\n",
    "config = {\n",
    "    \"image_size\": 336,                   # image size input to detector. Default=224\n",
    "    \"nms_threshold\": 0.5,                   # nms\n",
    "    \"conf_threshold\": 0.8,               # confidence\n",
    "    \"minface\": 50,                       # min face size. Default=50\n",
    "    \"device\": \"cpu\",                      # cpu, cuda\n",
    "    \"architecture\": \"mnet1\"               # res50 (ResNet 50), mnet1 (mobilenet1)\n",
    "}\n",
    "\n",
    "detector = RetinaFace(config)\n",
    "\n",
    "dummy_input = torch.zeros((1,3,config[\"image_size\"],config[\"image_size\"]))\n",
    "torch.onnx.export(detector.model, dummy_input, f'weights/{sdk_config[\"detector\"][\"architecture\"]}.onnx', opset_version=11)\n",
    "\n",
    "! python ~/intel/openvino_2021.3.394/deployment_tools/model_optimizer/mo_onnx.py --input_model=weights/mnet1.onnx --output_dir=weights \n",
    "# --data_type=FP16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/mnt/LINUXDATA/Source/object_tracking_v2/weights/res50.onnx\n",
      "\t- Path for generated IR: \t/mnt/LINUXDATA/Source/object_tracking_v2/weights\n",
      "\t- IR output name: \tres50\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tNone\n",
      "\t- Reverse input channels: \tFalse\n",
      "ONNX specific parameters:\n",
      "[ WARNING ] Failed to import Inference Engine Python API in: /home/khiemtv/intel/openvino_2021.3.394/python/python3.7\n",
      "[ WARNING ] libinference_engine.so: cannot open shared object file: No such file or directory\n",
      "\t- Inference Engine found in: \t/home/khiemtv/intel/openvino_2021.3.394/python/python3.7/openvino\n",
      "Inference Engine version: \t2.1.2021.3.0-2787-60059f2c755-releases/2021/3\n",
      "Model Optimizer version: \t    2021.3.0-2787-60059f2c755-releases/2021/3\n",
      "[ WARNING ]  \n",
      "Detected not satisfied dependencies:\n",
      "\ttest-generator: not installed, required: == 0.1.1\n",
      "\trequests: not installed, required: >= 2.20.0\n",
      "\n",
      "Please install required versions of components or use install_prerequisites script\n",
      "/home/khiemtv/intel/openvino_2021.3.394/deployment_tools/model_optimizer/install_prerequisites/install_prerequisites_onnx.sh\n",
      "Note that install_prerequisites scripts may install additional components.\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /mnt/LINUXDATA/Source/object_tracking_v2/weights/res50.xml\n",
      "[ SUCCESS ] BIN file: /mnt/LINUXDATA/Source/object_tracking_v2/weights/res50.bin\n",
      "[ SUCCESS ] Total execution time: 15.29 seconds. \n",
      "[ SUCCESS ] Memory consumed: 911 MB. \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from insight_face.modules.detection.retinaface.model_class import RetinaFace\n",
    "import cv2\n",
    "from core import support\n",
    "\n",
    "config = {\n",
    "    \"image_size\": 336,                   # image size input to detector. Default=224\n",
    "    \"nms_threshold\": 0.5,                   # nms\n",
    "    \"conf_threshold\": 0.8,               # confidence\n",
    "    \"minface\": 50,                       # min face size. Default=50\n",
    "    \"device\": \"cpu\",                      # cpu, cuda\n",
    "    \"architecture\": \"res50\"               # res50 (ResNet 50), mnet1 (mobilenet1)\n",
    "}\n",
    "\n",
    "detector = RetinaFace(config)\n",
    "\n",
    "dummy_input = torch.zeros((1,3,config[\"image_size\"],config[\"image_size\"]))\n",
    "torch.onnx.export(detector.model, dummy_input, f'weights/{config[\"architecture\"]}.onnx', opset_version=11)\n",
    "\n",
    "! python ~/intel/openvino_2021.3.394/deployment_tools/model_optimizer/mo_onnx.py --input_model=weights/res50.onnx --output_dir=weights \n",
    "# --data_type=FP16"
   ]
  },
  {
   "source": [
    "##### EXPORT FACE EMMBED"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/mnt/LINUXDATA/Source/object_tracking_v2/weights/iresnet34.onnx\n",
      "\t- Path for generated IR: \t/mnt/LINUXDATA/Source/object_tracking_v2/weights\n",
      "\t- IR output name: \tiresnet34\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tNone\n",
      "\t- Reverse input channels: \tFalse\n",
      "ONNX specific parameters:\n",
      "\t- Inference Engine found in: \t/home/khiemtv/anaconda3/envs/openvino/lib/python3.7/site-packages/openvino\n",
      "Inference Engine version: \t2.1.2021.3.0-2774-d6ebaa2cd8e-refs/pull/4731/head\n",
      "Model Optimizer version: \t    custom_dev_f1ca2af7f2041f4b2d7d51a34c0087a86030f01e\n",
      "[ WARNING ] Model Optimizer and Inference Engine versions do no match.\n",
      "[ WARNING ] Consider building the Inference Engine Python API from sources or reinstall OpenVINO (TM) toolkit using \"pip install openvino\" (may be incompatible with the current Model Optimizer version)\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /mnt/LINUXDATA/Source/object_tracking_v2/weights/iresnet34.xml\n",
      "[ SUCCESS ] BIN file: /mnt/LINUXDATA/Source/object_tracking_v2/weights/iresnet34.bin\n",
      "[ SUCCESS ] Total execution time: 9.79 seconds. \n",
      "[ SUCCESS ] Memory consumed: 990 MB. \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from insight_face.modules.recognition.insightface.insightface import InsightFaceEmbedder\n",
    "import cv2\n",
    "from core import support\n",
    "\n",
    "config = {\n",
    "    \"image_size\": 112,                   # input size of emmbeder \n",
    "    \"descriptor_size\": 512,\n",
    "    \"device\": \"cpu\",                      # cpu, cuda\n",
    "    \"architecture\": \"iresnet34\"           # iresnet100, iresnet50, iresnet34\n",
    "}\n",
    "\n",
    "face_embedder = InsightFaceEmbedder(config)\n",
    "\n",
    "dummy_input = torch.zeros((1,3,config[\"image_size\"],config[\"image_size\"]))\n",
    "torch.onnx.export(face_embedder.embedder, dummy_input, f'weights/{config[\"architecture\"]}.onnx', opset_version=11)\n",
    "\n",
    "! python ~/anaconda3/envs/openvino/lib/python3.7/site-packages/mo_onnx.py --input_model=weights/iresnet34.onnx --output_dir=weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# os.environ['LD_LIBRARY_PATH'] = \"/home/khiemtv/anaconda3/envs/openvino/lib\"\n",
    "# print(os.environ['LD_LIBRARY_PATH'])\n",
    "from openvino.inference_engine import IECore\n",
    "\n",
    "ie = IECore()\n",
    "net = ie.read_network(\"weights/iresnet34.xml\", \"weights/iresnet34.bin\")\n",
    "exec_net = ie.load_network(network=net, device_name=\"CPU\")\n",
    "image = np.zeros((1,3,112,112))\n",
    "# data[input_name] = images\n",
    "# print(face_embedder.embedder(dummy_input))\n",
    "\n",
    "ten_res = face_embedder.embedder(dummy_input).detach().numpy()\n",
    "res = exec_net.infer(inputs={\"input.1\": image})\n",
    "# print(res)\n",
    "# print(abs((ten_res - res['475'])/.sum())/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}