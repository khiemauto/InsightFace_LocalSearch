{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import json\n",
    "import threading\n",
    "import queue\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "# import redis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from core import support, share_param\n",
    "from insight_face.modules.tracking.custom_tracking import TrackingMultiCam\n",
    "from insight_face.utils.database import FaceRecognitionSystem\n",
    "from insight_face.modules.evalution.custom_evaluter import CustomEvaluter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "share_param.sdk_config = support.get_config_yaml(\"configs/sdk_config.yaml\")\n",
    "share_param.dev_config = support.get_config_yaml(\"configs/dev_config.yaml\")\n",
    "share_param.facerec_system = FaceRecognitionSystem(share_param.dev_config[\"DATA\"][\"photo_path\"], share_param.sdk_config )\n",
    "share_param.cam_infos = support.get_config_yaml(\"configs/cam_infos.yaml\") "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Check detect wheel\n",
    "share_param.facerec_system.sdk.create_database_notface_from_folders(\"dataset/notface\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "for filename in glob.glob(\"dataset/photos/175235458196/*.jpg\"):\n",
    "    photo = cv2.imread(filename)\n",
    "    photo = cv2.cvtColor(photo, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    photo = photo[photo.shape[0]//5: photo.shape[0]*4//5, photo.shape[1]//5: photo.shape[1]*4//5]\n",
    "    plt.figure()\n",
    "    plt.imshow(photo)\n",
    "    # cv2.imwrite(f\"crop_not_face_{filename}.jpg\", photo)\n",
    "    photo = cv2.resize(photo, (112,112))\n",
    "    descriptor = share_param.facerec_system.sdk.get_descriptor(photo)\n",
    "    indicies, distances = share_param.facerec_system.sdk.database_notface.find(descriptor, 1)\n",
    "\n",
    "    plt.title(f\"{indicies[0]} {distances[0]}\")\n",
    "    # print(indicies, distances)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for filename in glob.glob(\"/mnt/EXT4_DATA/LienVietTech/Data/showroom_check_quality/0pass/*.jpg\"):\n",
    "    bgr = cv2.imread(filename)\n",
    "    # bgr = bgr[bgr.shape[0]//5: bgr.shape[0] - bgr.shape[0]//5, bgr.shape[1]//5: bgr.shape[1] - bgr.shape[1]//5]\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    # preTime = time.time()\n",
    "    bboxes, landmarks = share_param.facerec_system.sdk.detect_post_faces(rgb)\n",
    "    # print(\"Detect\", time.time() - preTime)\n",
    "    # print(bboxes_batch)\n",
    "\n",
    "    bgrDraw = bgr.copy()\n",
    "\n",
    "    for bbox, landmark in zip(bboxes, landmarks):\n",
    "        cv2.rectangle(bgrDraw, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "        y = bbox[1] - 15 if bbox[1] - 15 > 15 else bbox[1] + 15\n",
    "        cv2.putText(bgrDraw, \"{:03.3f}\".format(bbox[4]), (int(bbox[0]), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "        cnts = landmark.reshape(5,2, order='F').astype(np.int16)\n",
    "        for point in cnts:\n",
    "            # print(point)\n",
    "            cv2.drawMarker(bgrDraw, tuple(point), color=(0,255,0), markerType=cv2.MARKER_CROSS, markerSize=8, thickness=2)\n",
    "        \n",
    "    if len(bboxes) > 0:\n",
    "        name = os.path.basename(filename)\n",
    "        cv2.imwrite(f\"datalab/pass/0.7_hasface/{name}\", bgrDraw)\n",
    "    else:\n",
    "        name = os.path.basename(filename)\n",
    "        cv2.imwrite(f\"datalab/pass/0.7_noface/{name}\", bgrDraw)\n",
    "    # plt.imshow(bgrDraw)\n",
    "# plt.show()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#CROP\n",
    "count = 0\n",
    "for root, dirs, files in os.walk(\"/mnt/LINUXDATA/Source/.data/face_attributes/mask02\", topdown=False):\n",
    "    # print(root)\n",
    "    for name in files:\n",
    "        count+=1\n",
    "        path = os.path.join(root, name)\n",
    "        bgr = cv2.imread(path)\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        bboxes, landmarks = share_param.facerec_system.sdk.detect_faces(rgb)\n",
    "        # print(bboxes)\n",
    "        # print(landmarks)\n",
    "        for bbox, landmark in zip(bboxes, landmarks):\n",
    "            faceW = bbox[2] - bbox[0]\n",
    "            faceH = bbox[3] - bbox[1]\n",
    "            expandLeft = max(0, bbox[0] - faceW/3)\n",
    "            expandTop = max(0, bbox[1] - faceH/3)\n",
    "            expandRight = min(bbox[2] + faceW/3, bgr.shape[1])\n",
    "            expandBottom = min(bbox[3] + faceH/3, bgr.shape[0])\n",
    "            faceCropExpand = bgr[int(expandTop):int(expandBottom), int(expandLeft):int(expandRight)].copy()\n",
    "            cv2.imwrite(f\"dataset/mask02/{count:04d}.jpg\", faceCropExpand)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check quality\n",
    "Blur and Straight"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "%matplotlib inline\n",
    "count = 0\n",
    "\n",
    "for cam_info in share_param.cam_infos[\"CamInfos\"]:\n",
    "    deviceID = cam_info[\"DeviceID\"]\n",
    "    camURL = cam_info[\"LinkRTSP\"]\n",
    "\n",
    "    evalution_config = share_param.sdk_config[\"evaluter\"]\n",
    "    evalution_config[\"illumination_threshold\"] = cam_info[\"illumination_threshold\"]\n",
    "    evalution_config[\"blur_threshold\"] = cam_info[\"blur_threshold\"]\n",
    "    share_param.evaluter_cams[deviceID] = CustomEvaluter(evalution_config)\n",
    "\n",
    "for root, dirs, files in os.walk(\"datalab/good\", topdown=False):\n",
    "    print(root)\n",
    "    for name in files:\n",
    "        count+=1\n",
    "        if count>100:\n",
    "            break\n",
    "        # print(name)\n",
    "        # if name != \"focus_02550.jpg\":\n",
    "        #     continue\n",
    "        path = os.path.join(root, name)\n",
    "        bgr = cv2.imread(path)\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        bboxes, landmarks = share_param.facerec_system.sdk.detect_faces(rgb)\n",
    "        # print(bboxes)\n",
    "        # print(landmarks)\n",
    "        for bbox, landmark in zip(bboxes, landmarks):\n",
    "            faceSize = float((bbox[3]-bbox[1])*(bbox[2]-bbox[0]))\n",
    "            faceAlign = share_param.facerec_system.sdk.align_face(rgb, landmark)\n",
    "            faceCrop = rgb[int(bbox[1]):int(bbox[3]),\n",
    "                                    int(bbox[0]):int(bbox[2])]\n",
    "\n",
    "            if faceCrop.shape[0] < 50 or faceCrop.shape[1] < 50:\n",
    "                continue\n",
    "\n",
    "            faceCrop = rgb[rgb.shape[0]//5: rgb.shape[0] - rgb.shape[0]//5, rgb.shape[1]//5: rgb.shape[1] - rgb.shape[1]//5]\n",
    "            # isillumination, threshillumination = share_param.facerec_system.sdk.evaluter.check_illumination(faceCrop)\n",
    "            # isNotBlur, threshnotblur = share_param.facerec_system.sdk.evaluter.check_not_blur(faceCrop)\n",
    "            # isStraightFace = share_param.facerec_system.sdk.evaluter.check_straight_face(rgb, landmark)\n",
    "\n",
    "            isillumination, threshillumination = share_param.evaluter_cams[1].check_illumination(faceCrop)\n",
    "            isNotBlur, threshnotblur = share_param.evaluter_cams[1].check_not_blur(faceCrop)\n",
    "\n",
    "\n",
    "            # isNotBlur, threshnotblur = share_param.facerec_system.sdk.evaluter.check_not_blur(faceCrop)\n",
    "            # illuminate = share_param.facerec_system.sdk.evaluter.check_illumination(faceCrop)\n",
    "            # isNotBlur, threshnotblur = share_param.facerec_system.sdk.evaluter.detect_blur_fft(faceAlign, size=30)\n",
    "            # isStraightFace = share_param.facerec_system.sdk.evaluter.check_straight_face(rgb, landmark)\n",
    "            # print(\"Not blur:\", isNotBlur)\n",
    "            # print(\"Straight:\", isStraightFace)\n",
    "            # cnts = landmark.reshape(5,2, order='F')\n",
    "            # for point in cnts:\n",
    "            #     cv2.drawMarker(rgb, tuple(point), color=(0,255,0), markerType=cv2.MARKER_CROSS, markerSize=8, thickness=1)\n",
    "            # cv2.putText()\n",
    "            plt.figure()\n",
    "            plt.title(f\"{name} {threshnotblur}\")\n",
    "            # cv2.putText(image, text, org, font, fontScale, color[, thickness[, lineType[, bottomLeftOrigin]]])\n",
    "            cv2.putText(bgr, \"{:03.3f}\".format(threshnotblur), (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "            new_path = os.path.join(root, \"blur_\"+ name)\n",
    "            cv2.imwrite(new_path, bgr)\n",
    "\n",
    "            plt.imshow(faceCrop)\n",
    "\n",
    "\n",
    "            # plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "#Check similar\n",
    "face_desc = {}\n",
    "for root, dirs, files in os.walk(\"datalab/similar\", topdown=False):\n",
    "    files.sort()\n",
    "    for name in files:\n",
    "        path = os.path.join(root, name)\n",
    "        bgr = cv2.imread(path)\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        # plt.imshow(rgb)\n",
    "\n",
    "        try:\n",
    "            preTime = time.time()\n",
    "            bboxes, landmarks = share_param.facerec_system.sdk.detect_post_faces(rgb)\n",
    "            print(time.time() - preTime)\n",
    "            if len(bboxes) > 1:\n",
    "                raise ValueError(\"Detected more than one face on provided image.\")\n",
    "            elif len(bboxes) == 0:\n",
    "                raise ValueError(\"Can't detect any faces on provided image.\")\n",
    "\n",
    "            cnts = landmarks[0].reshape(5,2, order='F').astype(np.int16)\n",
    "            for point in cnts:\n",
    "                cv2.drawMarker(rgb, tuple(point), color=(0,255,0), markerType=cv2.MARKER_CROSS, markerSize=8, thickness=2)\n",
    "\n",
    "            face = share_param.facerec_system.sdk.align_face(rgb, landmarks[0])\n",
    "            descriptor = share_param.facerec_system.sdk.get_descriptor(face)\n",
    "            plt.figure()\n",
    "            strInfo = f\"Score {bboxes[0][4]}\"\n",
    "            plt.title(strInfo)\n",
    "            plt.imshow(rgb)\n",
    "        except:\n",
    "            continue\n",
    "        face_desc[name] = descriptor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\n",
    "\n",
    "background = cv2.imread(\"datalab/background.png\")\n",
    "\n",
    "for root, dirs, files in os.walk(\"datalab/similar\", topdown=False):\n",
    "    for name in files:\n",
    "        path = os.path.join(root, name)\n",
    "        bgr = cv2.imread(path)\n",
    "\n",
    "        newbackground = background.copy()\n",
    "\n",
    "        newbackground[400:400+bgr.shape[0], 400:400+bgr.shape[1]] = bgr\n",
    "\n",
    "        bgrDraw = newbackground.copy()\n",
    "\n",
    "        rgb = cv2.cvtColor(newbackground, cv2.COLOR_BGR2RGB)\n",
    "        bboxes_batch, landmarks_batch = share_param.facerec_system.sdk.detect_faces_batch([rgb])\n",
    "\n",
    "        # bgrDraw = bgr.copy()\n",
    "\n",
    "        for bboxes, landmarks in zip(bboxes_batch, landmarks_batch):\n",
    "            for bbox, landmark in zip(bboxes, landmarks):\n",
    "                cv2.rectangle(bgrDraw, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "                y = bbox[1] - 15 if bbox[1] - 15 > 15 else bbox[1] + 15\n",
    "                cv2.putText(bgrDraw, \"{:03.3f}\".format(bbox[4]), (int(bbox[0]), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "                cnts = landmark.reshape(5,2, order='F').astype(np.int16)\n",
    "                for point in cnts:\n",
    "                    cv2.drawMarker(bgrDraw, tuple(point), color=(0,255,0), markerType=cv2.MARKER_CROSS, markerSize=8, thickness=2)\n",
    "        cv2.imwrite(\"datalab/output/\" + name, bgrDraw)\n",
    "        plt.imshow(bgrDraw)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import csv\n",
    "with open('names.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['name'] + list(face_desc)\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for cur_desc in face_desc:\n",
    "        row_simi = {'name': cur_desc}\n",
    "        for another_desc in face_desc:\n",
    "            simi = share_param.facerec_system.sdk.get_similarity(face_desc[cur_desc], face_desc[another_desc])\n",
    "            row_simi[another_desc] = f\"{simi:03.2f}\"\n",
    "\n",
    "        writer.writerow(row_simi)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('datalab/images/20210507145554353919.jpg')\n",
    "# img = cv2.resize(img, (224,224))\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "minsize = 20 # minimum size of face\n",
    "threshold = [ 0.6, 0.7, 0.7 ]  # three steps's threshold\n",
    "factor = 0.709 # scale factor\n",
    "\n",
    "#Size Parameter\n",
    "lower_threshold = 100\n",
    "upper_threshold = 200\n",
    "\n",
    "bboxes, points = share_param.facerec_system.sdk.detect_faces(img)\n",
    "\n",
    "print(bboxes, points)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def illumination(gray):\n",
    "    # length of R available  range  of  gray  intensities  excluding  5%  of  the darkest  and  brightest  pixel\n",
    "    sorted_gray = np.sort(gray.ravel())\n",
    "    l = len(sorted_gray)\n",
    "    cut_off_idx = l * 5 // 100\n",
    "    r = sorted_gray[l-cut_off_idx] - sorted_gray[cut_off_idx]\n",
    "    return np.round(r / 255, 2)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "illumination(gray)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_contour(pts):\n",
    "    return np.array([[pts[i], pts[5 + i]] for i in  [0, 1, 4, 3]], np.int32).reshape((-1,1,2))\n",
    "def get_mask(image, contour):\n",
    "    mask = np.zeros(image.shape[0:2],dtype=\"uint8\")\n",
    "    cv2.drawContours(mask, [contour], -1, 255, -1)\n",
    "    return mask"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sharpness(img, landmark):\n",
    "    contour = get_contour(landmark)\n",
    "    mask = get_mask(img, contour) #1-channel mask\n",
    "    # plt.imshow(mask)\n",
    "    mask = np.stack((mask,)*3, axis=-1) #3-channel mask\n",
    "    mask[mask == 255] = 1 # convert 0 and 255 to 0 and 1\n",
    "    laplacian = cv2.Laplacian(img,cv2.CV_64F)\n",
    "    # print(laplacian)\n",
    "    # print(laplacian.var())\n",
    "    edges = laplacian[mask.astype(bool)]\n",
    "    return np.round(edges.var() / 255 , 2)\n",
    "\n",
    "plt.imshow(img)\n",
    "sharpness(img, points[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from skimage.feature import hog\n",
    "\n",
    "def symmetry(img, landmark, bounding_box):\n",
    "    x1, y1, x2, y2 = int(min(bounding_box[0], min(landmark[:5]))), \\\n",
    "        int(min(bounding_box[1], min(landmark[5:]))), \\\n",
    "        int(max(bounding_box[2], max(landmark[:5]))), \\\n",
    "        int(max(bounding_box[3], max(landmark[5:])))\n",
    "    \n",
    "    landmark = np.array([[landmark[i], landmark[5 + i]] for i in  range(5)], np.int32).reshape((-1,1,2))\n",
    "    contour = landmark[:, 0] - [[x1, y1]]\n",
    "    \n",
    "    face = img[y1: y2, x1: x2].copy()\n",
    "    face_flip = cv2.flip(face, 1)\n",
    "    \n",
    "    fd_face, hog_face = hog(face, orientations=8, pixels_per_cell=(16, 16),\n",
    "                    cells_per_block=(1, 1), visualize=True, multichannel=True)\n",
    "    fd_flip, hog_flip = hog(face_flip, orientations=8, pixels_per_cell=(16, 16),\n",
    "                    cells_per_block=(1, 1), visualize=True, multichannel=True)\n",
    "    \n",
    "    \n",
    "    d = np.zeros(len(contour))\n",
    "    for i in range(len(d)):\n",
    "        d[i] = min(hog_face[contour[i, 1], contour[i, 0]], hog_flip[contour[i, 1], contour[i, 0]])\n",
    "    \n",
    "    return np.average(d)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "symmetry(img, points[0], bboxes[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure()\n",
    "for root, dirs, files in os.walk(\"datalab/images/\", topdown=False):\n",
    "    # print(root)\n",
    "    for name in files:\n",
    "        path = os.path.join(root, name)\n",
    "        img = cv2.imread(path)\n",
    "        # img = cv2.resize(img, (224,224))\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        bboxes, points = share_param.facerec_system.sdk.detect_faces(img)\n",
    "        if len(bboxes) == 0:\n",
    "            continue\n",
    "        # print(bboxes)\n",
    "        # print(points)\n",
    "        # print(\"illumination\",illumination(gray))\n",
    "        # print(\"sharpness\",sharpness(img, points[0]))\n",
    "        # print(\"symmetry\", symmetry(img, points[0], bboxes[0]))\n",
    "\n",
    "        cv2.rectangle(img, (int(bboxes[0][0]), int(bboxes[0][1])), (int(bboxes[0][2]), int(bboxes[0][3])), (0, 255, 0), 2)\n",
    "        plt.figure()\n",
    "        strInfo = f\"{illumination(gray)} {sharpness(img, points[0])} {symmetry(img, points[0], bboxes[0])}\"\n",
    "        plt.title(strInfo)\n",
    "        plt.imshow(img)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sharpness 0.27\n",
    "symmetry 0.11464042663574218"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TEST CARD ID"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import json\n",
    "import threading\n",
    "import queue\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import redis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from core import support, share_param\n",
    "# from insight_face.modules.tracking.custom_tracking import TrackingMultiCam\n",
    "# from insight_face.utils.database import FaceRecognitionSystem"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def prepare(image):\n",
    "    h= image.shape[0]\n",
    "    w= image.shape[1]\n",
    "\n",
    "    minsize = int(min(w,h)*2/3)\n",
    "\n",
    "    # print(w,h)\n",
    "    # print(minsize)\n",
    "\n",
    "    xstart = (w - minsize)//2\n",
    "    ystart = (h - minsize)//2\n",
    "    frame = image[ystart:ystart+minsize, xstart: xstart + minsize]\n",
    "    return frame\n",
    "\n",
    "def check_illumination(self, image):\n",
    "    if image is None or image.size == 0:\n",
    "        return False, 0.0\n",
    "    image = cv2.resize(image, (112,112))\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # length of R available  range  of  gray  intensities  excluding  5%  of  the darkest  and  brightest  pixel\n",
    "    sorted_gray = np.sort(gray.ravel())\n",
    "    l = len(sorted_gray)\n",
    "    cut_off_idx = l * 5 // 100\n",
    "    r = sorted_gray[l-cut_off_idx] - sorted_gray[cut_off_idx]\n",
    "    illuminate = np.round(r / 255, 2)\n",
    "\n",
    "    if illuminate < self.illumination_threshold:\n",
    "        return False, illuminate\n",
    "\n",
    "    return True, illuminate\n",
    "\n",
    "def check_illuminationhsv(self, image):\n",
    "    if image is None or image.size == 0:\n",
    "        return False, 0.0\n",
    "    image = cv2.resize(image, (112,112))\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    v = hsv[:,:,2]\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(v.ravel(),256,[0,256])\n",
    "    plt.show()\n",
    "\n",
    "    # sorted_gray = np.sort(v.ravel())\n",
    "    # l = len(sorted_gray)\n",
    "    # cut_off_idx = l * 5 // 100\n",
    "    # r = sorted_gray[l-cut_off_idx] - sorted_gray[cut_off_idx]\n",
    "    # illuminate = np.round(r / 255, 2)\n",
    "\n",
    "    illuminate = v.std()\n",
    "\n",
    "    if illuminate < self.illumination_threshold:\n",
    "        return False, illuminate\n",
    "\n",
    "    return True, illuminate   \n",
    "    # print(v)\n",
    "    # plt.figure()\n",
    "    # plt.hist(v.ravel(), bins=256, range=(0.0, 1.0), fc='k', ec='k') #calculating histogram\n",
    "\n",
    "    # print(v.shape)\n",
    "\n",
    "    # plt.figure()\n",
    "    # plt.title(\"brightvalue\")\n",
    "    # plt.imshow(v, cmap='gray')\n",
    "\n",
    "    return True , 10\n",
    "\n",
    "\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # length of R available  range  of  gray  intensities  excluding  5%  of  the darkest  and  brightest  pixel\n",
    "    sorted_gray = np.sort(gray.ravel())\n",
    "    l = len(sorted_gray)\n",
    "    cut_off_idx = l * 5 // 100\n",
    "    r = sorted_gray[l-cut_off_idx] - sorted_gray[cut_off_idx]\n",
    "    illuminate = np.round(r / 255, 2)\n",
    "\n",
    "    if illuminate < self.illumination_threshold:\n",
    "        return False, illuminate\n",
    "\n",
    "    return True, illuminate        \n",
    "\n",
    "def check_not_blur(self, image: np.ndarray):\n",
    "    if image is None or image.size == 0:\n",
    "        return False, 0.0\n",
    "\n",
    "    image = cv2.resize(image, (112,112))\n",
    "\n",
    "    real_notblur = cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "    standard_notblur = 300\n",
    "\n",
    "    threshnotblur = real_notblur/standard_notblur\n",
    "    # print(threshnotblur)\n",
    "\n",
    "    if threshnotblur < self.blur_threshold:\n",
    "        return False, threshnotblur\n",
    "    else:\n",
    "        return True, threshnotblur\n",
    "    return False, threshnotblur"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "count = 0\n",
    "\n",
    "class Param:\n",
    "    illumination_threshold = 0.8\n",
    "    blur_threshold = 0.8\n",
    "\n",
    "for root, dirs, files in os.walk(\"/mnt/LINUXDATA/Source/data/datacmt\", topdown=False):\n",
    "    if count>100:\n",
    "            break\n",
    "    for name in files:\n",
    "        count+=1\n",
    "        \n",
    "        path = os.path.join(root, name)\n",
    "        bgr = cv2.imread(path)\n",
    "        bgr = prepare(bgr)\n",
    "        # print(bgr.shape)\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        isNotBlur, threshnotblur = check_not_blur(Param, rgb)\n",
    "        illuminate, threshillumination = check_illuminationhsv(Param, rgb)\n",
    "        strR = \"{:03.3f} {:03.3f}\".format(threshnotblur, threshillumination)\n",
    "        pt1 = [10, 20]\n",
    "        fontFace = cv2.FONT_HERSHEY_COMPLEX_SMALL\n",
    "        fontScale = 0.5\n",
    "        thickness = 1\n",
    "        retval, baseLine = cv2.getTextSize(strR,fontFace=fontFace,fontScale=fontScale, thickness=thickness)\n",
    "        # Calculate the coordinates of the rectangular frame covering the text\n",
    "        topleft = (pt1[0], pt1[1] - retval[1])\n",
    "        bottomright = (topleft[0] + retval[0], topleft[1] + retval[1])\n",
    "        cv2.rectangle(rgb, (topleft[0], topleft[1] - baseLine), bottomright,thickness=-1, color=(0, 255, 0))\n",
    "        # Draw text\n",
    "        cv2.putText(rgb, strR, (pt1[0], pt1[1]-baseLine), fontScale=fontScale,fontFace=fontFace, thickness=thickness, color=(0,0,0))\n",
    "\n",
    "        # cv2.rectangle(bgr, (10, 10-textSize[1]), (10 + int(textSize[0][0]), 10 + int(textSize[0][1])), (0,0,0), -1)\n",
    "    \n",
    "        # cv2.putText(bgr, \"{:03.3f} {:03.3f}\".format(threshnotblur, threshillumination), (10,10-textSize[1]), cv2.FONT_HERSHEY_COMPLEX_SMALL, 0.5, (0,255,0), 1)\n",
    "        plt.figure()\n",
    "        plt.title(strR)\n",
    "        plt.imshow(rgb)\n",
    "        savename = \"R\"+name\n",
    "        cv2.imwrite(os.path.join(\"/mnt/LINUXDATA/Source/data/\", savename),cv2.cvtColor(rgb,cv2.COLOR_RGB2BGR))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### CONVERT JIT"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import json\n",
    "import threading\n",
    "import queue\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import redis\n",
    "import matplotlib.pyplot as plt\n",
    "from core import support\n",
    "\n",
    "from insight_face.modules.detection.retinaface.model_class import RetinaFace\n",
    "\n",
    "# from core import support, share_param\n",
    "# from insight_face.modules.tracking.custom_tracking import TrackingMultiCam\n",
    "# from insight_face.utils.database import FaceRecognitionSystem"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sdk_config = support.get_config_yaml(\"configs/sdk_config.yaml\")\n",
    "# share_param.dev_config = support.get_config_yaml(\"configs/dev_config.yaml\")\n",
    "# share_param.facerec_system = FaceRecognitionSystem(share_param.dev_config[\"DATA\"][\"photo_path\"], share_param.sdk_config )\n",
    "\n",
    "detector = RetinaFace(sdk_config[\"detector\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bgr = cv2.imread(\"dataset/bestphotos/113900549523G.jpg\")\n",
    "bgr = cv2.resize(bgr, (336,336))\n",
    "rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "# preTime = time.time()\n",
    "image = detector._preprocess(rgb)\n",
    "detector.model_input_shape = image.shape\n",
    "\n",
    "img = image.transpose((2, 0, 1))\n",
    "img = torch.from_numpy(img).unsqueeze(0)\n",
    "img = img.to(detector.device)\n",
    "# pred = detector.model(img)\n",
    "\n",
    "detector.model = torch.jit.trace(detector.model,img)\n",
    "# print(pred)\n",
    "\n",
    "# raw_pred = detector._predict_raw(image)\n",
    "\n",
    "# bgr = cv2.resize(bgr, (336,336))\n",
    "# bgr = cv2.resize(bgr, (336,336))\n",
    "# rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "# detector.predict(image)\n",
    "# traced_predict = torch.jit.trace(detector._predict_raw, image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "detector.model.half()\n",
    "for filename in glob.glob(\"/mnt/LINUXDATA/Source/.data/face_attributes/01000-20210525T030241Z-001/01000/*.png\"):\n",
    "    bgr = cv2.imread(filename)\n",
    "    bgr = cv2.resize(bgr, (336,336))\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    image = detector._preprocess(rgb)\n",
    "    detector.model_input_shape = image.shape\n",
    "    preTime = time.time()\n",
    "    # image.astype(np.float16)\n",
    "    raw_pred = detector._predict_raw(image)\n",
    "    print(\"Detect:\", time.time()-preTime)\n",
    "\n",
    "    bboxes, landms = detector._postprocess(raw_pred)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### EXPORT OPENVINO"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### EXPORT RETINA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from insight_face.modules.detection.retinaface.model_class import RetinaFace\n",
    "import cv2\n",
    "from core import support\n",
    "\n",
    "config = {\n",
    "    \"image_size\": 336,                   # image size input to detector. Default=224\n",
    "    \"nms_threshold\": 0.5,                   # nms\n",
    "    \"conf_threshold\": 0.8,               # confidence\n",
    "    \"minface\": 50,                       # min face size. Default=50\n",
    "    \"device\": \"cpu\",                      # cpu, cuda\n",
    "    \"architecture\": \"mnet1\"               # res50 (ResNet 50), mnet1 (mobilenet1)\n",
    "}\n",
    "\n",
    "detector = RetinaFace(config)\n",
    "\n",
    "dummy_input = torch.zeros((1,3,config[\"image_size\"],config[\"image_size\"]))\n",
    "torch.onnx.export(detector.model, dummy_input, f'weights/{sdk_config[\"detector\"][\"architecture\"]}.onnx', opset_version=11)\n",
    "\n",
    "! python ~/intel/openvino_2021.3.394/deployment_tools/model_optimizer/mo_onnx.py --input_model=weights/mnet1.onnx --output_dir=weights \n",
    "# --data_type=FP16"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from insight_face.modules.detection.retinaface.model_class import RetinaFace\n",
    "import cv2\n",
    "from core import support\n",
    "\n",
    "config = {\n",
    "    \"image_size\": 336,                   # image size input to detector. Default=224\n",
    "    \"nms_threshold\": 0.5,                   # nms\n",
    "    \"conf_threshold\": 0.8,               # confidence\n",
    "    \"minface\": 50,                       # min face size. Default=50\n",
    "    \"device\": \"cpu\",                      # cpu, cuda\n",
    "    \"architecture\": \"res50\"               # res50 (ResNet 50), mnet1 (mobilenet1)\n",
    "}\n",
    "\n",
    "detector = RetinaFace(config)\n",
    "\n",
    "dummy_input = torch.zeros((1,3,config[\"image_size\"],config[\"image_size\"]))\n",
    "torch.onnx.export(detector.model, dummy_input, f'weights/{config[\"architecture\"]}.onnx', opset_version=11)\n",
    "\n",
    "! python ~/intel/openvino_2021.3.394/deployment_tools/model_optimizer/mo_onnx.py --input_model=weights/res50.onnx --output_dir=weights \n",
    "# --data_type=FP16"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### EXPORT FACE EMMBED"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from insight_face.modules.recognition.insightface.insightface import InsightFaceEmbedder\n",
    "import cv2\n",
    "from core import support\n",
    "\n",
    "config = {\n",
    "    \"image_size\": 112,                   # input size of emmbeder \n",
    "    \"descriptor_size\": 512,\n",
    "    \"device\": \"cpu\",                      # cpu, cuda\n",
    "    \"architecture\": \"iresnet34\"           # iresnet100, iresnet50, iresnet34\n",
    "}\n",
    "\n",
    "face_embedder = InsightFaceEmbedder(config)\n",
    "\n",
    "dummy_input = torch.zeros((1,3,config[\"image_size\"],config[\"image_size\"]))\n",
    "torch.onnx.export(face_embedder.embedder, dummy_input, f'weights/{config[\"architecture\"]}.onnx', opset_version=11)\n",
    "\n",
    "! python ~/anaconda3/envs/openvino/lib/python3.7/site-packages/mo_onnx.py --input_model=weights/iresnet34.onnx --output_dir=weights "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from insight_face.modules.recognition.insightface.insightface import InsightFaceEmbedder\n",
    "import cv2\n",
    "from core import support\n",
    "\n",
    "config = {\n",
    "    \"image_size\": 112,                   # input size of emmbeder \n",
    "    \"descriptor_size\": 512,\n",
    "    \"device\": \"cpu\",                      # cpu, cuda\n",
    "    \"architecture\": \"iresnet50\"           # iresnet100, iresnet50, iresnet34\n",
    "}\n",
    "\n",
    "face_embedder = InsightFaceEmbedder(config)\n",
    "\n",
    "dummy_input = torch.zeros((1,3,config[\"image_size\"],config[\"image_size\"]))\n",
    "torch.onnx.export(face_embedder.embedder, dummy_input, f'weights/{config[\"architecture\"]}.onnx', opset_version=11)\n",
    "\n",
    "! python ~/anaconda3/envs/openvino/lib/python3.7/site-packages/mo_onnx.py --input_model=weights/iresnet50.onnx --output_dir=weights "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from insight_face.modules.recognition.insightface.insightface import InsightFaceEmbedder\n",
    "import cv2\n",
    "from core import support\n",
    "\n",
    "config = {\n",
    "    \"image_size\": 112,                   # input size of emmbeder \n",
    "    \"descriptor_size\": 512,\n",
    "    \"device\": \"cpu\",                      # cpu, cuda\n",
    "    \"architecture\": \"iresnet100\"           # iresnet100, iresnet50, iresnet34\n",
    "}\n",
    "\n",
    "face_embedder = InsightFaceEmbedder(config)\n",
    "\n",
    "dummy_input = torch.zeros((1,3,config[\"image_size\"],config[\"image_size\"]))\n",
    "torch.onnx.export(face_embedder.embedder, dummy_input, f'weights/{config[\"architecture\"]}.onnx', opset_version=11)\n",
    "\n",
    "! python ~/anaconda3/envs/openvino/lib/python3.7/site-packages/mo_onnx.py --input_model=weights/iresnet100.onnx --output_dir=weights "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# os.environ['LD_LIBRARY_PATH'] = \"/home/khiemtv/anaconda3/envs/openvino/lib\"\n",
    "# print(os.environ['LD_LIBRARY_PATH'])\n",
    "from openvino.inference_engine import IECore\n",
    "\n",
    "ie = IECore()\n",
    "net = ie.read_network(\"weights/iresnet34.xml\", \"weights/iresnet34.bin\")\n",
    "exec_net = ie.load_network(network=net, device_name=\"CPU\")\n",
    "image = np.zeros((1,3,112,112))\n",
    "# data[input_name] = images\n",
    "# print(face_embedder.embedder(dummy_input))\n",
    "\n",
    "ten_res = face_embedder.embedder(dummy_input).detach().numpy()\n",
    "res = exec_net.infer(inputs={\"input.1\": image})\n",
    "# print(res)\n",
    "# print(abs((ten_res - res['475'])/.sum())/)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def addFaceImageForCustomer(CIF_NO:str, Name:str, faceBase64: str) -> str:\n",
    "   date = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "   xmlReq = f'''\n",
    "   <soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" xmlns:fun=\"http://function.ws.core.gateway.com/\">\n",
    "      <soapenv:Header/>\n",
    "      <soapenv:Body>\n",
    "         <fun:execute>\n",
    "            <!--Optional:-->\n",
    "            <User>dangnt2</User>\n",
    "            <!--Optional:-->\n",
    "            <RequestData><![CDATA[\n",
    "\n",
    "   <FaceSearchRequest>\n",
    "      <RequestHeader>\n",
    "         <Command>faceRegister</Command>\n",
    "         <Password>dangnt@lienviet</Password>\n",
    "         <SystemTraceId>{date}</SystemTraceId>\n",
    "         <RequestDateTime>{date}</RequestDateTime>\n",
    "         <ChannelType>API</ChannelType>\n",
    "      </RequestHeader>\n",
    "      <FaceSearchRequestBody>\n",
    "         <ImageFace>\n",
    "            <Base64>{faceBase64}</Base64>\n",
    "         </ImageFace>\n",
    "         <UserIdentity>{CIF_NO}</UserIdentity>\n",
    "         <Name>{Name}</Name>\n",
    "         <Provider>LVT1</Provider>\n",
    "      </FaceSearchRequestBody>\n",
    "   </FaceSearchRequest>\n",
    "\n",
    "            ]]></RequestData>\n",
    "            <!--Optional:-->\n",
    "            <RequestKey></RequestKey>\n",
    "         </fun:execute>\n",
    "      </soapenv:Body>\n",
    "   </soapenv:Envelope>\n",
    "   '''\n",
    "\n",
    "def opencv_to_base64(image: np.ndarray) -> str:\n",
    "    if image is None or image.size == 0:\n",
    "        raise ValueError(\"image empty!\")\n",
    "    retval, buffer = cv2.imencode(\".png\", image)\n",
    "    bas64img = base64.b64encode(buffer).decode(\"utf-8\")\n",
    "    return bas64img\n",
    "\n",
    "image_cv = cv2.imread(\"/home/khiemtv/Desktop/SMART_QUEUE.jpg\")\n",
    "base64_image = opencv_to_base64(image_cv)\n",
    "\n",
    "retValueTemp = addFaceImageForCustomer(base64_image)\n",
    "\n",
    "   \n",
    "x = requests.post(\"http://10.36.126.112/services/ekyc\", data = retValueTemp, headers = {\"Content-Type\": \"text/xml; charset=utf-8\", \"SOAPAction\":\"\"}, timeout=60)\n",
    "   \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import requests\n",
    "import cv2\n",
    "import numpy as np\n",
    "import base64\n",
    "\n",
    "URL_API = \"http://10.36.209.66:6688/esmac/services/EsmacService?wsdl\"\n",
    "\n",
    "def opencv_to_base64(image: np.ndarray) -> str:\n",
    "    if image is None or image.size == 0:\n",
    "        raise ValueError(\"image empty!\")\n",
    "    # image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    retval, buffer = cv2.imencode(\".jpg\", image)\n",
    "    bas64img = base64.b64encode(buffer).decode(\"utf-8\")\n",
    "    return bas64img\n",
    "\n",
    "image_cv = cv2.imread(\"/home/khiemtv/Desktop/TraMy.png\")\n",
    "base64_image = opencv_to_base64(image_cv)\n",
    "print(type(base64_image))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from datetime import datetime\n",
    "from typing import List\n",
    "def getDataSoapClient(faceBase64: List[str]) -> str:\n",
    "   print(len(faceBase64))\n",
    "   date = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")[:-3]\n",
    "   # date = LocalDateTime.now().format(DateTimeFormatter.ofPattern(\"yyyyMMddHHmmss\"));//20200207104330\n",
    "   xmlReq = \"<soapenv:Envelope xmlns:soapenv=\\\"http://schemas.xmlsoap.org/soap/envelope/\\\" xmlns:esm=\\\"http://esmac.ewallet.lpb.com\\\" xmlns:xsd=\\\"http://request.showroom.ewallet.lpb.com/xsd\\\" xmlns:xsd1=\\\"http://common.entity.ewallet.lpb.com/xsd\\\">\\r\\n\" \\\n",
    "            \"   <soapenv:Header/>\\r\\n\" \\\n",
    "            \"   <soapenv:Body>\\r\\n\" \\\n",
    "            \"      <esm:getSmartCustVip>\\r\\n\" \\\n",
    "            \"         <!--Optional:-->\\r\\n\" \\\n",
    "            \"         <esm:request>\\r\\n\" \\\n",
    "            \"            <!--Optional:-->\\r\\n\" \\\n",
    "            \"            <xsd:header>\\r\\n\" \\\n",
    "            \"               <!--Optional:-->\\r\\n\" \\\n",
    "            \"               <xsd1:channelCode>M</xsd1:channelCode>\\r\\n\" \\\n",
    "            \"               <!--Optional:-->\\r\\n\" \\\n",
    "            \"               <xsd1:deviceId>khiemtv</xsd1:deviceId>\\r\\n\" \\\n",
    "            \"               <!--Optional:-->\\r\\n\" \\\n",
    "            \"               <xsd1:ip>127.0.0.1</xsd1:ip>\\r\\n\" \\\n",
    "            \"               <xsd1:txnId>\"+date+\"</xsd1:txnId>\\r\\n\" \\\n",
    "            \"               <!--Optional:-->\\r\\n\" \\\n",
    "            \"               <xsd1:txnTime>\"+date+\"</xsd1:txnTime>\\r\\n\" \\\n",
    "            \"               <xsd1:userName>khiemtv</xsd1:userName>\\r\\n\" \\\n",
    "            \"            </xsd:header>\\r\\n\" \\\n",
    "            \"            <!--Zero or more repetitions:-->\\r\\n\"\n",
    "   \n",
    "   for img in faceBase64:\n",
    "      xmlReq+=\"         <xsd:imgBase64>\"+img+\"</xsd:imgBase64>\\r\\n\"\n",
    "\n",
    "   xmlReq+=\"         </esm:request>\\r\\n\"\n",
    "   xmlReq+=\"      </esm:getSmartCustVip>\\r\\n\"\n",
    "   xmlReq+=\"   </soapenv:Body>\\r\\n\"\n",
    "   xmlReq+=\"</soapenv:Envelope>\"\n",
    "\n",
    "   return xmlReq\n",
    "\n",
    "soap_format = '''<soapenv:Envelope xmlns:soapenv=\"http://schemas.xmlsoap.org/soap/envelope/\" xmlns:esm=\"http://esmac.ewallet.lpb.com\" xmlns:xsd=\"http://request.ewallet.lpb.com/xsd\" xmlns:xsd1=\"http://common.entity.ewallet.lpb.com/xsd\">\n",
    "   <soapenv:Header/>\n",
    "   <soapenv:Body>\n",
    "      <esm:faceSearch>\n",
    "         <!--Optional:-->\n",
    "         <esm:request>\n",
    "            <!--Optional:-->\n",
    "            <xsd:header>\n",
    "               <!--Optional:-->\n",
    "               <xsd1:channelCode>M</xsd1:channelCode>\n",
    "               <!--Optional:-->\n",
    "               <xsd1:deviceId>khiemtv</xsd1:deviceId>\n",
    "               <!--Optional:-->\n",
    "               <xsd1:ip>127.0.0.1</xsd1:ip>\n",
    "               <!--Optional:-->\n",
    "               <xsd1:txnId>{txnId}</xsd1:txnId>\n",
    "               <!--Optional:-->\n",
    "               <xsd1:txnTime>{txnTime}</xsd1:txnTime>\n",
    "               <!--Optional:-->\n",
    "               <xsd1:userName>khiemtv</xsd1:userName>\n",
    "            </xsd:header>\n",
    "            <!--Optional:-->\n",
    "            <xsd:imageFace>{base64}</xsd:imageFace>\n",
    "         </esm:request>\n",
    "      </esm:faceSearch>\n",
    "   </soapenv:Body>\n",
    "</soapenv:Envelope>'''\n",
    "\n",
    "def get_soap_message(base64_img: str) -> str:\n",
    "    txnId = datetime.now().strftime(\"%Y%m%d%H%M%S%f\")[:-3]\n",
    "    txnTime = txnId\n",
    "    soap_message = soap_format.format(txnId=txnId, txnTime=txnTime, base64=base64_img)\n",
    "    return soap_message\n",
    "\n",
    "retValueTemp = getDataSoapClient([base64_image])\n",
    "# retValueTemp = get_soap_message(base64_image)\n",
    "\n",
    "# print(retValueTemp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x = requests.post(URL_API, data = retValueTemp, headers = {\"Content-Type\": \"text/xml; charset=utf-8\", \"SOAPAction\":\"\"})\n",
    "# print(x.text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(x.text)\n",
    "import xml.etree.ElementTree as ET\n",
    "# dom = ET.fromstring(x.content)\n",
    "\n",
    "# define namespace mappings to use as shorthand below\n",
    "namespaces = {\n",
    "    'ax233': 'http://entity.showroom.ewallet.lpb.com/xsd',\n",
    "    'ax214': 'http://entity.ewallet.lpb.com/xsd',\n",
    "    'ax216': \"http://entity.showroom.ewallet.lpb.com/xsd\"\n",
    "}\n",
    "dom = ET.fromstring(x.content)\n",
    "\n",
    "# print(dom.find('.//{http://entity.showroom.ewallet.lpb.com/xsd}resultDesc').text)\n",
    "\n",
    "# reference the namespace mappings here by `<name>:`\n",
    "customerNames = dom.findall(\n",
    "    './/ax216:fullName',\n",
    "    namespaces\n",
    ")\n",
    "\n",
    "scores = dom.findall(\n",
    "    './/ax216:score',\n",
    "    namespaces\n",
    ")\n",
    "\n",
    "recordStatus = dom.findall(\n",
    "    './/ax216:recordStatus',\n",
    "    namespaces\n",
    ")\n",
    "\n",
    "# for record in recordStatus:\n",
    "#     print(record)\n",
    "#     print(record.text.isdigit())\n",
    "\n",
    "# print(name.text)\n",
    "for name, score in zip(customerNames, scores):\n",
    "    print(name, score)\n",
    "    print(name.text)\n",
    "    print(score.text)\n",
    "    print(score.text.isnumeric())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "names = dom.findall(\n",
    "    './/ax233:imgPath',\n",
    "    namespaces\n",
    ")\n",
    "# print(name.text)\n",
    "for name in names:\n",
    "    print(name.text)\n",
    "    print(os.path.splitext(os.path.basename(name.text))[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install cx_Oracle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "names = dom.findall(\n",
    "    './/ax233:imgPath',\n",
    "    namespaces\n",
    ")\n",
    "# print(name.text)\n",
    "request_id = \"\"\n",
    "for name in names:\n",
    "    print(name.text)\n",
    "    request_id = os.path.splitext(os.path.basename(name.text))[0]\n",
    "\n",
    "print(request_id)\n",
    "\n",
    "import cx_Oracle\n",
    "\n",
    "connection = cx_Oracle.connect(user=\"ESMAC_VV_TE\", password=\"esmac_vv_te\", dsn=\"10.36.126.42:1521/VIVIET\")\n",
    "cursor = connection.cursor()\n",
    "\n",
    "\n",
    "statement = f'UPDATE SMART_QUEUE SET RECORD_STATUS = \\'C\\' WHERE REQUEST_ID = {request_id}'\n",
    "cursor.execute(statement)\n",
    "connection.commit()\n",
    "\n",
    "for row in cursor.execute(f'SELECT RECORD_STATUS FROM SMART_QUEUE WHERE REQUEST_ID = {request_id}'):\n",
    "    print(row)\n",
    "\n",
    "# for row in cursor.execute(f'select RECORD_STATUS from SMART_QUEUE where REQUEST_ID = 3775'):\n",
    "#     print(row)\n",
    "\n",
    "# sql = ('update SMART_QUEUE '\n",
    "#         'set RECORD_STATUS = :RECORD_STATUS '\n",
    "#         'where REQUEST_ID = :REQUEST_ID')\n",
    "\n",
    "# RECORD_STATUS = \"C\"\n",
    "# REQUEST_ID = 3775\n",
    "# cursor.execute(sql, [RECORD_STATUS, REQUEST_ID])\n",
    "\n",
    "# ret = cursor.execute(\"UPDATE SMART_QUEUE SET RECORD_STATUS = 'C' WHERE REQUEST_ID = 3775\")\n",
    "# print(ret)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install gTTS pydub\n",
    "!sudo apt install libout123-0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from gtts import gTTS\n",
    "from io import BytesIO\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "tts = gTTS(text='Xin chào, Trần Khiêm', lang='vi')\n",
    "fp = BytesIO()\n",
    "tts.write_to_fp(fp)\n",
    "fp.seek(0)\n",
    "\n",
    "song = AudioSegment.from_file(fp, format=\"mp3\")\n",
    "play(song)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb82d6d48d7233739249b9bbd11e945fb3ddd6c6e3fac1392f125b3f2246243c"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('object_tracking_v2': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "2f1f12eb66b03e8d9695162c79e98253b80e970c6ca8f0e78ea223164db03af3"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}