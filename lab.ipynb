{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0392a3ab2b76076cdcc2cc84a7c6cfadfd0fb8ef882f2999646c8c4f08e895f6c",
   "display_name": "Python 3.9.4 64-bit ('object_tracking_v2': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "392a3ab2b76076cdcc2cc84a7c6cfadfd0fb8ef882f2999646c8c4f08e895f6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import json\n",
    "import threading\n",
    "import queue\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import redis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from core import support, share_param\n",
    "from insight_face.modules.tracking.custom_tracking import TrackingMultiCam\n",
    "from insight_face.utils.database import FaceRecognitionSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_param.sdk_config = support.get_config_yaml(\"configs/sdk_config.yaml\")\n",
    "share_param.dev_config = support.get_config_yaml(\"configs/dev_config.yaml\")\n",
    "share_param.facerec_system = FaceRecognitionSystem(share_param.dev_config[\"DATA\"][\"photo_path\"], share_param.sdk_config )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_not_blur(image: np.ndarray):\n",
    "    if image is None or image.size == 0:\n",
    "        return False, 0.0\n",
    "\n",
    "    # gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    imageresize = cv2.resize(image, (112,112))\n",
    "    real_notblur = cv2.Laplacian(imageresize, cv2.CV_64F).var()\n",
    "    standard_notblur = 300\n",
    "    threshnotblur = real_notblur/standard_notblur\n",
    "    print(threshnotblur)\n",
    "\n",
    "    if threshnotblur < 1.5:\n",
    "        return False, threshnotblur\n",
    "    else:\n",
    "        return True, threshnotblur\n",
    "    return False, threshnotblur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def illumination(image):\n",
    "    image = cv2.resize(image, (112,112))\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # length of R available  range  of  gray  intensities  excluding  5%  of  the darkest  and  brightest  pixel\n",
    "    sorted_gray = np.sort(gray.ravel())\n",
    "    l = len(sorted_gray)\n",
    "    cut_off_idx = l * 5 // 100\n",
    "    r = sorted_gray[l-cut_off_idx] - sorted_gray[cut_off_idx]\n",
    "    return np.round(r / 255, 2)"
   ]
  },
  {
   "source": [
    "### Check quality\n",
    "Blur and Straight"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "count = 0\n",
    "for root, dirs, files in os.walk(\"dataset/bestphotos\", topdown=False):\n",
    "    # print(root)\n",
    "    for name in files:\n",
    "        count+=1\n",
    "        if count>100:\n",
    "            break\n",
    "        # print(name)\n",
    "        # if name != \"focus_02550.jpg\":\n",
    "        #     continue\n",
    "        path = os.path.join(root, name)\n",
    "        rgb = cv2.imread(path)\n",
    "        # rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
    "        bboxes, landmarks = share_param.facerec_system.sdk.detect_faces(rgb)\n",
    "        # print(bboxes)\n",
    "        # print(landmarks)\n",
    "        for bbox, landmark in zip(bboxes, landmarks):\n",
    "            faceSize = float((bbox[3]-bbox[1])*(bbox[2]-bbox[0]))\n",
    "            faceAlign = share_param.facerec_system.sdk.align_face(rgb, landmark)\n",
    "            faceCrop = rgb[int(bbox[1]):int(bbox[3]),\n",
    "                                    int(bbox[0]):int(bbox[2])]\n",
    "\n",
    "            if faceCrop.shape[0] < 50 or faceCrop.shape[1] < 50:\n",
    "                continue\n",
    "            isillumination, threshillumination = share_param.facerec_system.sdk.evaluter.check_illumination(faceCrop)\n",
    "            isNotBlur, threshnotblur = share_param.facerec_system.sdk.evaluter.check_not_blur(faceCrop)\n",
    "            isStraightFace = share_param.facerec_system.sdk.evaluter.check_straight_face(rgb, landmark)\n",
    "\n",
    "            # isNotBlur, threshnotblur = share_param.facerec_system.sdk.evaluter.check_not_blur(faceCrop)\n",
    "            # illuminate = share_param.facerec_system.sdk.evaluter.check_illumination(faceCrop)\n",
    "            # isNotBlur, threshnotblur = share_param.facerec_system.sdk.evaluter.detect_blur_fft(faceAlign, size=30)\n",
    "            # isStraightFace = share_param.facerec_system.sdk.evaluter.check_straight_face(rgb, landmark)\n",
    "            # print(\"Not blur:\", isNotBlur)\n",
    "            # print(\"Straight:\", isStraightFace)\n",
    "            cnts = landmark.reshape(5,2, order='F')\n",
    "            # for point in cnts:\n",
    "            #     cv2.drawMarker(rgb, tuple(point), color=(0,255,0), markerType=cv2.MARKER_CROSS, markerSize=8, thickness=1)\n",
    "            plt.figure()\n",
    "            plt.title(f\"{name} {threshillumination} {threshnotblur} {isStraightFace}\")\n",
    "            plt.imshow(faceCrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('datalab/images/20210507145554353919.jpg')\n",
    "# img = cv2.resize(img, (224,224))\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "minsize = 20 # minimum size of face\n",
    "threshold = [ 0.6, 0.7, 0.7 ]  # three steps's threshold\n",
    "factor = 0.709 # scale factor\n",
    "\n",
    "#Size Parameter\n",
    "lower_threshold = 100\n",
    "upper_threshold = 200\n",
    "\n",
    "bboxes, points = share_param.facerec_system.sdk.detect_faces(img)\n",
    "\n",
    "print(bboxes, points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def illumination(gray):\n",
    "    # length of R available  range  of  gray  intensities  excluding  5%  of  the darkest  and  brightest  pixel\n",
    "    sorted_gray = np.sort(gray.ravel())\n",
    "    l = len(sorted_gray)\n",
    "    cut_off_idx = l * 5 // 100\n",
    "    r = sorted_gray[l-cut_off_idx] - sorted_gray[cut_off_idx]\n",
    "    return np.round(r / 255, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "illumination(gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contour(pts):\n",
    "    return np.array([[pts[i], pts[5 + i]] for i in  [0, 1, 4, 3]], np.int32).reshape((-1,1,2))\n",
    "def get_mask(image, contour):\n",
    "    mask = np.zeros(image.shape[0:2],dtype=\"uint8\")\n",
    "    cv2.drawContours(mask, [contour], -1, 255, -1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpness(img, landmark):\n",
    "    contour = get_contour(landmark)\n",
    "    mask = get_mask(img, contour) #1-channel mask\n",
    "    # plt.imshow(mask)\n",
    "    mask = np.stack((mask,)*3, axis=-1) #3-channel mask\n",
    "    mask[mask == 255] = 1 # convert 0 and 255 to 0 and 1\n",
    "    laplacian = cv2.Laplacian(img,cv2.CV_64F)\n",
    "    # print(laplacian)\n",
    "    # print(laplacian.var())\n",
    "    edges = laplacian[mask.astype(bool)]\n",
    "    return np.round(edges.var() / 255 , 2)\n",
    "\n",
    "plt.imshow(img)\n",
    "sharpness(img, points[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "\n",
    "def symmetry(img, landmark, bounding_box):\n",
    "    x1, y1, x2, y2 = int(min(bounding_box[0], min(landmark[:5]))), \\\n",
    "        int(min(bounding_box[1], min(landmark[5:]))), \\\n",
    "        int(max(bounding_box[2], max(landmark[:5]))), \\\n",
    "        int(max(bounding_box[3], max(landmark[5:])))\n",
    "    \n",
    "    landmark = np.array([[landmark[i], landmark[5 + i]] for i in  range(5)], np.int32).reshape((-1,1,2))\n",
    "    contour = landmark[:, 0] - [[x1, y1]]\n",
    "    \n",
    "    face = img[y1: y2, x1: x2].copy()\n",
    "    face_flip = cv2.flip(face, 1)\n",
    "    \n",
    "    fd_face, hog_face = hog(face, orientations=8, pixels_per_cell=(16, 16),\n",
    "                    cells_per_block=(1, 1), visualize=True, multichannel=True)\n",
    "    fd_flip, hog_flip = hog(face_flip, orientations=8, pixels_per_cell=(16, 16),\n",
    "                    cells_per_block=(1, 1), visualize=True, multichannel=True)\n",
    "    \n",
    "    \n",
    "    d = np.zeros(len(contour))\n",
    "    for i in range(len(d)):\n",
    "        d[i] = min(hog_face[contour[i, 1], contour[i, 0]], hog_flip[contour[i, 1], contour[i, 0]])\n",
    "    \n",
    "    return np.average(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symmetry(img, points[0], bboxes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for root, dirs, files in os.walk(\"datalab/images/\", topdown=False):\n",
    "    # print(root)\n",
    "    for name in files:\n",
    "        path = os.path.join(root, name)\n",
    "        img = cv2.imread(path)\n",
    "        # img = cv2.resize(img, (224,224))\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        bboxes, points = share_param.facerec_system.sdk.detect_faces(img)\n",
    "        if len(bboxes) == 0:\n",
    "            continue\n",
    "        # print(bboxes)\n",
    "        # print(points)\n",
    "        # print(\"illumination\",illumination(gray))\n",
    "        # print(\"sharpness\",sharpness(img, points[0]))\n",
    "        # print(\"symmetry\", symmetry(img, points[0], bboxes[0]))\n",
    "\n",
    "        cv2.rectangle(img, (int(bboxes[0][0]), int(bboxes[0][1])), (int(bboxes[0][2]), int(bboxes[0][3])), (0, 255, 0), 2)\n",
    "        plt.figure()\n",
    "        strInfo = f\"{illumination(gray)} {sharpness(img, points[0])} {symmetry(img, points[0], bboxes[0])}\"\n",
    "        plt.title(strInfo)\n",
    "        plt.imshow(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpness 0.27\n",
    "symmetry 0.11464042663574218"
   ]
  },
  {
   "source": [
    "### TEST CARD ID"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import json\n",
    "import threading\n",
    "import queue\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import redis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from core import support, share_param\n",
    "# from insight_face.modules.tracking.custom_tracking import TrackingMultiCam\n",
    "# from insight_face.utils.database import FaceRecognitionSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(image):\n",
    "    h= image.shape[0]\n",
    "    w= image.shape[1]\n",
    "\n",
    "    minsize = int(min(w,h)*2/3)\n",
    "\n",
    "    # print(w,h)\n",
    "    # print(minsize)\n",
    "\n",
    "    xstart = (w - minsize)//2\n",
    "    ystart = (h - minsize)//2\n",
    "    frame = image[ystart:ystart+minsize, xstart: xstart + minsize]\n",
    "    return frame\n",
    "\n",
    "def check_illumination(self, image):\n",
    "    if image is None or image.size == 0:\n",
    "        return False, 0.0\n",
    "    image = cv2.resize(image, (112,112))\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # length of R available  range  of  gray  intensities  excluding  5%  of  the darkest  and  brightest  pixel\n",
    "    sorted_gray = np.sort(gray.ravel())\n",
    "    l = len(sorted_gray)\n",
    "    cut_off_idx = l * 5 // 100\n",
    "    r = sorted_gray[l-cut_off_idx] - sorted_gray[cut_off_idx]\n",
    "    illuminate = np.round(r / 255, 2)\n",
    "\n",
    "    if illuminate < self.illumination_threshold:\n",
    "        return False, illuminate\n",
    "\n",
    "    return True, illuminate\n",
    "\n",
    "def check_illuminationhsv(self, image):\n",
    "    if image is None or image.size == 0:\n",
    "        return False, 0.0\n",
    "    image = cv2.resize(image, (112,112))\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    v = hsv[:,:,2]\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(v.ravel(),256,[0,256])\n",
    "    plt.show()\n",
    "\n",
    "    # sorted_gray = np.sort(v.ravel())\n",
    "    # l = len(sorted_gray)\n",
    "    # cut_off_idx = l * 5 // 100\n",
    "    # r = sorted_gray[l-cut_off_idx] - sorted_gray[cut_off_idx]\n",
    "    # illuminate = np.round(r / 255, 2)\n",
    "\n",
    "    illuminate = v.std()\n",
    "\n",
    "    if illuminate < self.illumination_threshold:\n",
    "        return False, illuminate\n",
    "\n",
    "    return True, illuminate   \n",
    "    # print(v)\n",
    "    # plt.figure()\n",
    "    # plt.hist(v.ravel(), bins=256, range=(0.0, 1.0), fc='k', ec='k') #calculating histogram\n",
    "\n",
    "    # print(v.shape)\n",
    "\n",
    "    # plt.figure()\n",
    "    # plt.title(\"brightvalue\")\n",
    "    # plt.imshow(v, cmap='gray')\n",
    "\n",
    "    return True , 10\n",
    "\n",
    "\n",
    "\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # length of R available  range  of  gray  intensities  excluding  5%  of  the darkest  and  brightest  pixel\n",
    "    sorted_gray = np.sort(gray.ravel())\n",
    "    l = len(sorted_gray)\n",
    "    cut_off_idx = l * 5 // 100\n",
    "    r = sorted_gray[l-cut_off_idx] - sorted_gray[cut_off_idx]\n",
    "    illuminate = np.round(r / 255, 2)\n",
    "\n",
    "    if illuminate < self.illumination_threshold:\n",
    "        return False, illuminate\n",
    "\n",
    "    return True, illuminate        \n",
    "\n",
    "def check_not_blur(self, image: np.ndarray):\n",
    "    if image is None or image.size == 0:\n",
    "        return False, 0.0\n",
    "\n",
    "    image = cv2.resize(image, (112,112))\n",
    "\n",
    "    real_notblur = cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "    standard_notblur = 300\n",
    "\n",
    "    threshnotblur = real_notblur/standard_notblur\n",
    "    # print(threshnotblur)\n",
    "\n",
    "    if threshnotblur < self.blur_threshold:\n",
    "        return False, threshnotblur\n",
    "    else:\n",
    "        return True, threshnotblur\n",
    "    return False, threshnotblur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "class Param:\n",
    "    illumination_threshold = 0.8\n",
    "    blur_threshold = 0.8\n",
    "\n",
    "for root, dirs, files in os.walk(\"/mnt/LINUXDATA/Source/data/datacmt\", topdown=False):\n",
    "    if count>100:\n",
    "            break\n",
    "    for name in files:\n",
    "        count+=1\n",
    "        \n",
    "        path = os.path.join(root, name)\n",
    "        bgr = cv2.imread(path)\n",
    "        bgr = prepare(bgr)\n",
    "        # print(bgr.shape)\n",
    "        rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "        isNotBlur, threshnotblur = check_not_blur(Param, rgb)\n",
    "        illuminate, threshillumination = check_illuminationhsv(Param, rgb)\n",
    "        strR = \"{:03.3f} {:03.3f}\".format(threshnotblur, threshillumination)\n",
    "        pt1 = [10, 20]\n",
    "        fontFace = cv2.FONT_HERSHEY_COMPLEX_SMALL\n",
    "        fontScale = 0.5\n",
    "        thickness = 1\n",
    "        retval, baseLine = cv2.getTextSize(strR,fontFace=fontFace,fontScale=fontScale, thickness=thickness)\n",
    "        # Calculate the coordinates of the rectangular frame covering the text\n",
    "        topleft = (pt1[0], pt1[1] - retval[1])\n",
    "        bottomright = (topleft[0] + retval[0], topleft[1] + retval[1])\n",
    "        cv2.rectangle(rgb, (topleft[0], topleft[1] - baseLine), bottomright,thickness=-1, color=(0, 255, 0))\n",
    "        # Draw text\n",
    "        cv2.putText(rgb, strR, (pt1[0], pt1[1]-baseLine), fontScale=fontScale,fontFace=fontFace, thickness=thickness, color=(0,0,0))\n",
    "\n",
    "        # cv2.rectangle(bgr, (10, 10-textSize[1]), (10 + int(textSize[0][0]), 10 + int(textSize[0][1])), (0,0,0), -1)\n",
    "    \n",
    "        # cv2.putText(bgr, \"{:03.3f} {:03.3f}\".format(threshnotblur, threshillumination), (10,10-textSize[1]), cv2.FONT_HERSHEY_COMPLEX_SMALL, 0.5, (0,255,0), 1)\n",
    "        plt.figure()\n",
    "        plt.title(strR)\n",
    "        plt.imshow(rgb)\n",
    "        savename = \"R\"+name\n",
    "        cv2.imwrite(os.path.join(\"/mnt/LINUXDATA/Source/data/\", savename),cv2.cvtColor(rgb,cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "source": [
    "#### CONVERT JIT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import json\n",
    "import threading\n",
    "import queue\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import redis\n",
    "import matplotlib.pyplot as plt\n",
    "from core import support\n",
    "\n",
    "from insight_face.modules.detection.retinaface.model_class import RetinaFace\n",
    "\n",
    "# from core import support, share_param\n",
    "# from insight_face.modules.tracking.custom_tracking import TrackingMultiCam\n",
    "# from insight_face.utils.database import FaceRecognitionSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdk_config = support.get_config_yaml(\"configs/sdk_config.yaml\")\n",
    "# share_param.dev_config = support.get_config_yaml(\"configs/dev_config.yaml\")\n",
    "# share_param.facerec_system = FaceRecognitionSystem(share_param.dev_config[\"DATA\"][\"photo_path\"], share_param.sdk_config )\n",
    "\n",
    "detector = RetinaFace(sdk_config[\"detector\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgr = cv2.imread(\"dataset/bestphotos/113900549523G.jpg\")\n",
    "bgr = cv2.resize(bgr, (336,336))\n",
    "rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "# preTime = time.time()\n",
    "image = detector._preprocess(rgb)\n",
    "detector.model_input_shape = image.shape\n",
    "\n",
    "img = image.transpose((2, 0, 1))\n",
    "img = torch.from_numpy(img).unsqueeze(0)\n",
    "img = img.to(detector.device)\n",
    "# pred = detector.model(img)\n",
    "\n",
    "detector.model = torch.jit.trace(detector.model,img)\n",
    "# print(pred)\n",
    "\n",
    "# raw_pred = detector._predict_raw(image)\n",
    "\n",
    "# bgr = cv2.resize(bgr, (336,336))\n",
    "# bgr = cv2.resize(bgr, (336,336))\n",
    "# rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "# detector.predict(image)\n",
    "# traced_predict = torch.jit.trace(detector._predict_raw, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector.model.half()\n",
    "for filename in glob.glob(\"/mnt/LINUXDATA/Source/.data/face_attributes/01000-20210525T030241Z-001/01000/*.png\"):\n",
    "    bgr = cv2.imread(filename)\n",
    "    bgr = cv2.resize(bgr, (336,336))\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    image = detector._preprocess(rgb)\n",
    "    detector.model_input_shape = image.shape\n",
    "    preTime = time.time()\n",
    "    # image.astype(np.float16)\n",
    "    raw_pred = detector._predict_raw(image)\n",
    "    print(\"Detect:\", time.time()-preTime)\n",
    "\n",
    "    bboxes, landms = detector._postprocess(raw_pred)\n"
   ]
  },
  {
   "source": [
    "#### EXPORT OPENVINO"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from insight_face.modules.detection.retinaface.model_class import RetinaFace\n",
    "import cv2\n",
    "from core import support\n",
    "\n",
    "sdk_config = support.get_config_yaml(\"configs/sdk_config.yaml\")\n",
    "\n",
    "detector = RetinaFace(sdk_config[\"detector\"])\n",
    "\n",
    "bgr = cv2.imread(\"\")\n",
    "bgr = cv2.resize(bgr, (sdk_config[\"detector\"][\"image_size\"],sdk_config[\"detector\"][\"image_size\"]))\n",
    "rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "image = detector._preprocess(rgb)\n",
    "detector.model_input_shape = image.shape\n",
    "\n",
    "img = image.transpose((2, 0, 1))\n",
    "dummy_input = torch.from_numpy(img).unsqueeze(0)\n",
    "output = detector.model(dummy_input)\n",
    "\n",
    "out_cus = []\n",
    "for out in output:\n",
    "    out_cus.append(out.detach().numpy())\n",
    "\n",
    "    \n",
    "# print(out_cus)\n",
    "# print(output[0].shape, output[1].shape, output[2].shape)\n",
    "# torch.onnx.export(detector.model, dummy_input, 'weights/mnet1.onnx', opset_version=11)\n",
    "torch.onnx.export(detector.model, dummy_input, 'weights/mnet1.onnx', opset_version=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ~/intel/openvino_2021.3.394/deployment_tools/model_optimizer/mo_onnx.py --input_model weights/res50.onnx --output_dir weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "resnet50model = torchvision.models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50model.eval()\n",
    "# Create dummy input for the model. It will be used to run the model inside export function. \n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "# Call the export function\n",
    "torch.onnx.export(resnet50model, (dummy_input, ), 'resnet50model.onnx', opset_version=11, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}